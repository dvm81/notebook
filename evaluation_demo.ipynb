{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persona Summarization Evaluation - Demonstration\n",
    "\n",
    "This notebook walks through the evaluation strategy, demonstrating each metric with concrete examples.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Data Loading](#setup)\n",
    "2. [ROUGE Metrics](#rouge)\n",
    "3. [BERTScore](#bertscore)\n",
    "4. [BLEURT](#bleurt)\n",
    "5. [Stylometric Features](#style)\n",
    "6. [Complete Evaluation](#complete)\n",
    "7. [Persona Comparison](#persona)\n",
    "8. [Summary & Insights](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 1. Setup & Data Loading\n",
    "\n",
    "First, let's load our test data and set up the evaluation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our modules\n",
    "from src.io_utils import load_jsonl, load_persona_assignments\n",
    "from src.text_utils import tokenize_sentences, tokenize_words, count_tokens\n",
    "from src.content_metrics import ContentMetricsCalculator\n",
    "from src.style_features import StyleAnalyzer\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load test data\n",
    "records = list(load_jsonl('data/input.jsonl'))\n",
    "persona_map = load_persona_assignments('data/persona_assignments.csv')\n",
    "\n",
    "print(f\"‚úì Loaded {len(records)} articles\")\n",
    "print(f\"‚úì Loaded {len(persona_map)} persona assignments\")\n",
    "print(f\"\\nPersonas: {list(set(persona_map.values()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our first example\n",
    "example = records[0]\n",
    "\n",
    "print(\"üì∞ Article Title:\", example['document_title'])\n",
    "print(\"üìù Sector:\", example['metadata']['sector'])\n",
    "print(\"üë§ Author:\", example['metadata']['author'])\n",
    "print(\"üé≠ Assigned Persona:\", persona_map.get(example['write_id']))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüìÑ SOURCE (first 300 chars):\")\n",
    "print(example['document_content'][:300] + \"...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüéØ GOLD SUMMARY:\")\n",
    "print(example['expected_summary'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nü§ñ GENERATED SUMMARY:\")\n",
    "print(example['agented_summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rouge'></a>\n",
    "## 2. ROUGE Metrics\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures n-gram overlap between generated and reference summaries.\n",
    "\n",
    "### What each ROUGE variant measures:\n",
    "- **ROUGE-1**: Unigram (single word) overlap - measures basic word coverage\n",
    "- **ROUGE-2**: Bigram (two consecutive words) overlap - measures phrase preservation\n",
    "- **ROUGE-L**: Longest common subsequence - measures sentence-level structure similarity\n",
    "\n",
    "### Metrics explained:\n",
    "- **Precision**: What % of words in generated summary are in reference?\n",
    "- **Recall**: What % of words in reference are captured in generated summary?\n",
    "- **F1**: Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE for our first example\n",
    "reference = example['expected_summary']\n",
    "generated = example['agented_summary']\n",
    "\n",
    "scores = scorer.score(reference, generated)\n",
    "\n",
    "print(\"üìä ROUGE Scores for Example 1:\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "for metric_name, metric_scores in scores.items():\n",
    "    print(f\"\\n{metric_name.upper()}:\")\n",
    "    print(f\"  Precision: {metric_scores.precision:.4f}\")\n",
    "    print(f\"  Recall:    {metric_scores.recall:.4f}\")\n",
    "    print(f\"  F1:        {metric_scores.fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the scores:\n",
    "\n",
    "Let's visualize what these numbers mean by looking at the actual word overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and compare\n",
    "ref_words = set(reference.lower().split())\n",
    "gen_words = set(generated.lower().split())\n",
    "\n",
    "overlap_words = ref_words & gen_words\n",
    "only_in_ref = ref_words - gen_words\n",
    "only_in_gen = gen_words - ref_words\n",
    "\n",
    "print(\"üìù Word-level Analysis:\")\n",
    "print(f\"\\nReference summary words: {len(ref_words)}\")\n",
    "print(f\"Generated summary words: {len(gen_words)}\")\n",
    "print(f\"Overlapping words: {len(overlap_words)}\")\n",
    "print(f\"\\n‚úì Words in BOTH: {len(overlap_words)} words\")\n",
    "print(f\"Examples: {list(overlap_words)[:15]}\")\n",
    "print(f\"\\n‚ö† Only in REFERENCE: {len(only_in_ref)} words\")\n",
    "print(f\"Examples: {list(only_in_ref)[:10]}\")\n",
    "print(f\"\\n‚ö† Only in GENERATED: {len(only_in_gen)} words\")\n",
    "print(f\"Examples: {list(only_in_gen)[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare multiple examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROUGE for first 5 examples\n",
    "rouge_results = []\n",
    "\n",
    "for i in range(min(5, len(records))):\n",
    "    rec = records[i]\n",
    "    scores = scorer.score(rec['expected_summary'], rec['agented_summary'])\n",
    "    \n",
    "    rouge_results.append({\n",
    "        'article': rec['document_title'][:40] + '...',\n",
    "        'persona': persona_map.get(rec['write_id']),\n",
    "        'rouge1_f': scores['rouge1'].fmeasure,\n",
    "        'rouge2_f': scores['rouge2'].fmeasure,\n",
    "        'rougeLsum_f': scores['rougeLsum'].fmeasure,\n",
    "    })\n",
    "\n",
    "df_rouge = pd.DataFrame(rouge_results)\n",
    "print(\"\\nüìä ROUGE F1 Scores Comparison:\\n\")\n",
    "print(df_rouge.to_string(index=False))\n",
    "print(f\"\\nüìà Average Scores:\")\n",
    "print(f\"ROUGE-1:    {df_rouge['rouge1_f'].mean():.4f}\")\n",
    "print(f\"ROUGE-2:    {df_rouge['rouge2_f'].mean():.4f}\")\n",
    "print(f\"ROUGE-Lsum: {df_rouge['rougeLsum_f'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bertscore'></a>\n",
    "## 3. BERTScore\n",
    "\n",
    "BERTScore uses contextual embeddings from BERT to measure semantic similarity.\n",
    "\n",
    "**Why BERTScore?**\n",
    "- ROUGE only matches exact words\n",
    "- BERTScore understands synonyms and paraphrasing\n",
    "- Example: \"car\" and \"automobile\" get high BERTScore but ROUGE=0\n",
    "\n",
    "**Note:** BERTScore requires downloading models and can be slow on first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "try:\n    import bert_score\n    \n    print(\"Calculating BERTScore (this may take a minute on first run)...\\n\")\n    \n    # Use roberta-large for better quality (from config)\n    bertscore_model = config['content'].get('bertscore_model', 'roberta-large')\n    print(f\"Using model: {bertscore_model}\\n\")\n    \n    P, R, F1 = bert_score.score(\n        [example['agented_summary']], \n        [example['expected_summary']],\n        model_type=bertscore_model,\n        lang='en',\n        rescale_with_baseline=True,\n        verbose=False\n    )\n    \n    print(f\"üìä BERTScore for Example 1:\")\n    print(f\"  Precision: {P.item():.4f}\")\n    print(f\"  Recall:    {R.item():.4f}\")\n    print(f\"  F1:        {F1.item():.4f}\")\n    \n    print(f\"\\nüí° Interpretation:\")\n    print(f\"BERTScore F1 of {F1.item():.4f} indicates {'excellent' if F1.item() > 0.9 else 'good' if F1.item() > 0.85 else 'fair'} semantic similarity\")\n    print(f\"\\nCompare to ROUGE-1 F1: {scores['rouge1'].fmeasure:.4f}\")\n    print(\"BERTScore is typically higher because it captures paraphrasing and synonyms.\")\n    \n    bertscore_available = True\n    \nexcept Exception as e:\n    print(f\"‚ö† BERTScore demo skipped: {e}\")\n    print(\"Install with: pip install bert-score\")\n    bertscore_available = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTScore Example: Paraphrasing Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate BERTScore advantage over ROUGE\nref_text = \"The company reported strong revenue growth in the third quarter.\"\ngen_text1 = \"The firm posted robust sales increases during Q3.\"  # Paraphrase\ngen_text2 = \"The weather was nice yesterday afternoon.\"  # Unrelated\n\n# ROUGE scores\nrouge1 = scorer.score(ref_text, gen_text1)['rouge1'].fmeasure\nrouge2 = scorer.score(ref_text, gen_text2)['rouge1'].fmeasure\n\nprint(\"üî¨ Comparing ROUGE vs BERTScore:\\n\")\nprint(\"Reference: 'The company reported strong revenue growth in the third quarter.'\\n\")\nprint(f\"Text 1 (paraphrase): '{gen_text1}'\")\nprint(f\"  ROUGE-1 F1: {rouge1:.4f}\")\n\nif bertscore_available:\n    try:\n        P1, R1, F1_1 = bert_score.score([gen_text1], [ref_text], \n                                        model_type=bertscore_model, \n                                        lang='en',\n                                        verbose=False)\n        print(f\"  BERTScore F1: {F1_1.item():.4f} ‚úì Correctly identifies semantic similarity\\n\")\n    except:\n        print(\"  BERTScore: [calculation error]\\n\")\nelse:\n    print(\"  BERTScore: [not available]\\n\")\n\nprint(f\"Text 2 (unrelated): '{gen_text2}'\")\nprint(f\"  ROUGE-1 F1: {rouge2:.4f}\")\n\nif bertscore_available:\n    try:\n        P2, R2, F1_2 = bert_score.score([gen_text2], [ref_text], \n                                        model_type=bertscore_model,\n                                        lang='en',\n                                        verbose=False)\n        print(f\"  BERTScore F1: {F1_2.item():.4f} ‚úì Correctly identifies no similarity\")\n    except:\n        print(\"  BERTScore: [calculation error]\")\nelse:\n    print(\"  BERTScore: [not available]\")\n\nprint(\"\\nüí° BERTScore handles paraphrasing much better than ROUGE!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bleurt'></a>\n",
    "## 4. BLEURT\n",
    "\n",
    "BLEURT (Bilingual Evaluation Understudy with Representations from Transformers) is a learned metric trained on human judgments.\n",
    "\n",
    "**Why BLEURT?**\n",
    "- Trained to match human quality assessments\n",
    "- Considers fluency, coherence, and meaning\n",
    "- Scores typically range from -1 (poor) to +1 (excellent)\n",
    "- Correlates better with human judgments than ROUGE\n",
    "\n",
    "**Note:** BLEURT requires TensorFlow and downloads large models (~1GB). First run will be slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Check if BLEURT is enabled in config\nbleurt_enabled = config['content'].get('use_bleurt', False)\nbleurt_checkpoint = config['content'].get('bleurt_checkpoint', 'bleurt_checkpoints/BLEURT-20-D3')\n\nif not bleurt_enabled:\n    print(\"‚ö† BLEURT is disabled in config.yaml\")\n    print(f\"\\nBLEURT requires:\")\n    print(\"  - TensorFlow (heavy dependency)\")\n    print(\"  - Large checkpoint download (~300MB-1GB)\")\n    print(\"  - May have compatibility issues on some systems (especially macOS)\")\n    print(f\"\\nTo enable BLEURT:\")\n    print(\"  1. Set use_bleurt: true in config.yaml\")\n    print(\"  2. Run: python setup_bleurt.py\")\n    print(\"  3. Ensure checkpoint exists at: {bleurt_checkpoint}\")\n    print(f\"\\nüìñ See BLEURT_SETUP.md for detailed instructions\")\n    print(f\"\\nFor this demo, we'll continue with ROUGE + BERTScore (which work great!)\")\n    bleurt_available = False\nelse:\n    try:\n        from bleurt import score\n        \n        # Suppress TensorFlow warnings\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n        \n        print(f\"Initializing BLEURT scorer with checkpoint: {bleurt_checkpoint}...\")\n        print(\"‚è≥ This may take a minute...\\n\")\n        \n        # Initialize BLEURT with downloaded checkpoint\n        bleurt_scorer = score.BleurtScorer(bleurt_checkpoint)\n        \n        print(f\"‚úì BLEURT scorer initialized successfully!\\n\")\n        \n        # Calculate BLEURT score for our example\n        bleurt_scores = bleurt_scorer.score(\n            references=[example['expected_summary']],\n            candidates=[example['agented_summary']]\n        )\n        \n        bleurt_score = bleurt_scores[0]\n        \n        print(f\"üìä BLEURT Score for Example 1:\")\n        print(f\"  Score: {bleurt_score:.4f}\")\n        \n        print(f\"\\nüí° Interpretation:\")\n        if bleurt_score > 0.6:\n            interpretation = \"excellent quality - very similar to reference\"\n        elif bleurt_score > 0.4:\n            interpretation = \"good quality - captures main content well\"\n        elif bleurt_score > 0.2:\n            interpretation = \"fair quality - some content overlap\"\n        else:\n            interpretation = \"needs improvement - limited similarity\"\n        \n        print(f\"BLEURT score of {bleurt_score:.4f} indicates {interpretation}\")\n        \n        print(f\"\\nüìà Comparison to other metrics:\")\n        print(f\"  ROUGE-1 F1: {scores['rouge1'].fmeasure:.4f}\")\n        print(f\"  ROUGE-Lsum F1: {scores['rougeLsum'].fmeasure:.4f}\")\n        if bertscore_available:\n            print(f\"  BERTScore F1: {F1.item():.4f}\")\n        print(f\"  BLEURT: {bleurt_score:.4f}\")\n        \n        print(\"\\nüí° BLEURT is trained on human judgments, so it often captures quality nuances\")\n        print(\"   that pure overlap metrics like ROUGE might miss.\")\n        \n        bleurt_available = True\n        \n    except ImportError as e:\n        print(\"‚ö† BLEURT package not installed\")\n        print(f\"\\nError: {e}\")\n        print(\"\\nTo install BLEURT:\")\n        print(\"  pip install 'git+https://github.com/google-research/bleurt.git'\")\n        print(\"  pip install 'tensorflow>=2.0.0'\")\n        bleurt_available = False\n        \n    except FileNotFoundError as e:\n        print(f\"‚ö† BLEURT checkpoint not found: {bleurt_checkpoint}\")\n        print(f\"\\nError: {e}\")\n        print(\"\\nTo download checkpoint:\")\n        print(\"  python setup_bleurt.py\")\n        print(\"\\nOr manually download from:\")\n        print(\"  https://github.com/google-research/bleurt#checkpoints\")\n        bleurt_available = False\n        \n    except RuntimeError as e:\n        error_str = str(e)\n        if 'mutex' in error_str.lower() or 'lock' in error_str.lower():\n            print(\"‚ö† BLEURT initialization failed due to system compatibility issue\")\n            print(f\"\\nError: TensorFlow mutex/threading issue (common on some macOS configurations)\")\n            print(\"\\nThis is a known TensorFlow compatibility issue, not a problem with your setup.\")\n            print(\"\\nüìä Good news: ROUGE + BERTScore provide excellent evaluation without BLEURT!\")\n            print(\"\\nWorkarounds if you need BLEURT:\")\n            print(\"  - Use Linux or Docker environment\")\n            print(\"  - Try: pip install tensorflow==2.10.0\")\n            print(\"  - See BLEURT_SETUP.md for details\")\n        else:\n            print(f\"‚ö† BLEURT initialization failed: {e}\")\n        bleurt_available = False\n        \n    except Exception as e:\n        print(f\"‚ö† BLEURT calculation failed: {e}\")\n        print(\"\\nBLEURT can be challenging to set up. Common issues:\")\n        print(\"  - TensorFlow version incompatibility\")\n        print(\"  - Model checkpoint not found\")\n        print(\"  - System compatibility (macOS mutex errors)\")\n        print(f\"\\nüìñ See BLEURT_SETUP.md for troubleshooting\")\n        print(f\"\\nFor this demo, we'll continue with ROUGE + BERTScore.\")\n        bleurt_available = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEURT vs ROUGE: Why the difference?\n",
    "\n",
    "BLEURT is trained to predict human ratings, so it can recognize:\n",
    "- **Fluency**: Natural-sounding text scores higher\n",
    "- **Coherence**: Logical flow matters\n",
    "- **Semantic equivalence**: Different words, same meaning\n",
    "- **Context**: Understands what information is important\n",
    "\n",
    "ROUGE only counts word overlap, so it might:\n",
    "- Give high scores to awkward but word-matching text\n",
    "- Give low scores to excellent paraphrases\n",
    "- Miss semantic understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='style'></a>\n",
    "## 5. Stylometric Features & Persona Fidelity\n",
    "\n",
    "Beyond content accuracy, we evaluate how well the summary matches the target persona's writing style.\n",
    "\n",
    "### Stylometric Features Extracted:\n",
    "1. **Function word rate** - Common words (the, is, at, etc.)\n",
    "2. **Average sentence length** - Words per sentence\n",
    "3. **Type-token ratio** - Vocabulary diversity\n",
    "4. **Punctuation patterns** - Comma, period, exclamation usage\n",
    "5. **Pronoun rate** - Personal pronoun frequency\n",
    "6. **Flesch-Kincaid grade** - Reading difficulty\n",
    "7. **Average word length** - Character count per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize style analyzer\n",
    "style_analyzer = StyleAnalyzer(config)\n",
    "\n",
    "# Extract features from our example summary\n",
    "summary_features = style_analyzer.extract_stylometric_features(example['agented_summary'])\n",
    "\n",
    "feature_names = [\n",
    "    'Function word rate',\n",
    "    'Avg sentence length (norm)',\n",
    "    'Type-token ratio',\n",
    "    'Comma rate',\n",
    "    'Period rate',\n",
    "    'Exclamation rate',\n",
    "    'Question rate',\n",
    "    'Pronoun rate',\n",
    "    'FK grade (norm)',\n",
    "    'Avg word length (norm)'\n",
    "]\n",
    "\n",
    "print(\"üìê Stylometric Features for Generated Summary:\\n\")\n",
    "print(\"=\"*60)\n",
    "for name, value in zip(feature_names, summary_features):\n",
    "    print(f\"{name:30s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Across Personas\n",
    "\n",
    "Let's see how different personas have distinctive stylometric signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build persona centroids\n",
    "print(\"Building persona centroids from training samples...\\n\")\n",
    "centroids = style_analyzer.build_persona_centroids(force_rebuild=True)\n",
    "\n",
    "print(f\"‚úì Built centroids for {len(centroids)} personas\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display persona characteristics\n",
    "for persona_id, centroid in centroids.items():\n",
    "    print(f\"\\nüé≠ {persona_id.upper()} Persona:\")\n",
    "    print(f\"  Function words: {centroid[0]:.4f}\")\n",
    "    print(f\"  Sentence length: {centroid[1]:.4f}\")\n",
    "    print(f\"  Vocabulary diversity: {centroid[2]:.4f}\")\n",
    "    print(f\"  Exclamation rate: {centroid[5]:.4f}\")\n",
    "    print(f\"  Pronoun rate: {centroid[7]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persona Similarity Calculation\n",
    "\n",
    "We use Jensen-Shannon divergence to measure how similar a summary is to each persona's style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity to each persona for our example\n",
    "summary_text = example['agented_summary']\n",
    "target_persona = persona_map.get(example['write_id'])\n",
    "\n",
    "print(f\"üìÑ Summary from article: {example['document_title']}\")\n",
    "print(f\"üéØ Target persona: {target_persona}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìä Style Similarity to Each Persona:\\n\")\n",
    "\n",
    "similarities = {}\n",
    "for persona_id in centroids.keys():\n",
    "    similarity = style_analyzer.calculate_style_similarity(summary_text, persona_id)\n",
    "    similarities[persona_id] = similarity\n",
    "    marker = \"‚úì\" if persona_id == target_persona else \" \"\n",
    "    print(f\"  {marker} {persona_id:20s}: {similarity:.4f}\")\n",
    "\n",
    "best_match = max(similarities, key=similarities.get)\n",
    "print(f\"\\nüèÜ Best match: {best_match}\")\n",
    "print(f\"{'‚úì Correct!' if best_match == target_persona else '‚úó Mismatch'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Examples by Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example summaries from each persona\n",
    "persona_examples = {}\n",
    "for rec in records[:9]:  # First 9 records\n",
    "    persona = persona_map.get(rec['write_id'])\n",
    "    if persona and persona not in persona_examples:\n",
    "        persona_examples[persona] = rec['agented_summary']\n",
    "\n",
    "print(\"‚úçÔ∏è Example Summaries by Persona:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for persona, text in persona_examples.items():\n",
    "    print(f\"\\nüé≠ {persona.upper()}:\")\n",
    "    print(f\"{text[:200]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='complete'></a>\n",
    "## 6. Complete Evaluation\n",
    "\n",
    "Now let's run the complete evaluation on all articles with all available metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complete evaluation with ROUGE, BERTScore (optional), BLEURT (optional), and Style\nresults = []\n\nprint(\"Running complete evaluation on all articles...\\n\")\nprint(f\"Metrics enabled:\")\nprint(f\"  ‚úì ROUGE\")\nprint(f\"  {'‚úì' if bertscore_available else '‚úó'} BERTScore ({bertscore_model if bertscore_available else 'N/A'})\")\nprint(f\"  {'‚úì' if bleurt_available else '‚úó'} BLEURT\")\nprint(f\"  ‚úì Stylometric Similarity\\n\")\n\nfor i, rec in enumerate(records):\n    print(f\"Processing {i+1}/{len(records)}: {rec['document_title'][:50]}...\")\n    \n    source = rec['document_content']\n    reference = rec['expected_summary']\n    generated = rec['agented_summary']\n    persona = persona_map.get(rec['write_id'])\n    \n    # ROUGE scores\n    rouge_scores = scorer.score(reference, generated)\n    \n    # BERTScore (if available)\n    bert_f1 = None\n    if bertscore_available:\n        try:\n            _, _, bert_F = bert_score.score([generated], [reference], \n                                           model_type=bertscore_model,\n                                           lang='en',\n                                           rescale_with_baseline=True,\n                                           verbose=False)\n            bert_f1 = bert_F.item()\n        except:\n            pass\n    \n    # BLEURT (if available)\n    bleurt_val = None\n    if bleurt_available:\n        try:\n            bleurt_scores = bleurt_scorer.score(references=[reference], candidates=[generated])\n            bleurt_val = bleurt_scores[0]\n        except:\n            pass\n    \n    # Style similarity\n    style_sim = style_analyzer.calculate_style_similarity(generated, persona)\n    \n    # Token counts\n    src_tokens = count_tokens(source)\n    ref_tokens = count_tokens(reference)\n    gen_tokens = count_tokens(generated)\n    \n    results.append({\n        'article': rec['document_title'][:40] + '...',\n        'sector': rec['metadata']['sector'],\n        'persona': persona,\n        'rouge1_f': rouge_scores['rouge1'].fmeasure,\n        'rouge2_f': rouge_scores['rouge2'].fmeasure,\n        'rougeLsum_f': rouge_scores['rougeLsum'].fmeasure,\n        'bertscore_f1': bert_f1,\n        'bleurt': bleurt_val,\n        'style_sim': style_sim,\n        'compression': gen_tokens / src_tokens if src_tokens > 0 else 0,\n        'src_tokens': src_tokens,\n        'gen_tokens': gen_tokens\n    })\n\ndf_results = pd.DataFrame(results)\nprint(\"\\n‚úì Evaluation complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nüìä EVALUATION RESULTS\\n\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Show available metrics\n",
    "display_cols = ['article', 'persona', 'rouge1_f', 'rouge2_f', 'rougeLsum_f']\n",
    "if bertscore_available:\n",
    "    display_cols.append('bertscore_f1')\n",
    "if bleurt_available:\n",
    "    display_cols.append('bleurt')\n",
    "display_cols.append('style_sim')\n",
    "\n",
    "print(df_results[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nüìà SUMMARY STATISTICS\\n\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nContent Quality:\")\n",
    "print(f\"  ROUGE-1 F1:    {df_results['rouge1_f'].mean():.4f} ¬± {df_results['rouge1_f'].std():.4f}\")\n",
    "print(f\"  ROUGE-2 F1:    {df_results['rouge2_f'].mean():.4f} ¬± {df_results['rouge2_f'].std():.4f}\")\n",
    "print(f\"  ROUGE-Lsum F1: {df_results['rougeLsum_f'].mean():.4f} ¬± {df_results['rougeLsum_f'].std():.4f}\")\n",
    "\n",
    "if bertscore_available and df_results['bertscore_f1'].notna().any():\n",
    "    print(f\"  BERTScore F1:  {df_results['bertscore_f1'].mean():.4f} ¬± {df_results['bertscore_f1'].std():.4f}\")\n",
    "\n",
    "if bleurt_available and df_results['bleurt'].notna().any():\n",
    "    print(f\"  BLEURT:        {df_results['bleurt'].mean():.4f} ¬± {df_results['bleurt'].std():.4f}\")\n",
    "\n",
    "print(f\"\\nStyle Fidelity:\")\n",
    "print(f\"  Style Similarity: {df_results['style_sim'].mean():.4f} ¬± {df_results['style_sim'].std():.4f}\")\n",
    "\n",
    "print(f\"\\nCompression:\")\n",
    "print(f\"  Avg compression ratio: {df_results['compression'].mean():.2%}\")\n",
    "print(f\"  Avg source tokens: {df_results['src_tokens'].mean():.0f}\")\n",
    "print(f\"  Avg generated tokens: {df_results['gen_tokens'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='persona'></a>\n",
    "## 7. Persona Comparison\n",
    "\n",
    "Let's analyze performance by persona to see if style matching is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by persona\n",
    "agg_dict = {\n",
    "    'rouge1_f': ['mean', 'std', 'count'],\n",
    "    'rougeLsum_f': ['mean', 'std'],\n",
    "    'style_sim': ['mean', 'std']\n",
    "}\n",
    "\n",
    "if bertscore_available and df_results['bertscore_f1'].notna().any():\n",
    "    agg_dict['bertscore_f1'] = ['mean', 'std']\n",
    "\n",
    "if bleurt_available and df_results['bleurt'].notna().any():\n",
    "    agg_dict['bleurt'] = ['mean', 'std']\n",
    "\n",
    "persona_stats = df_results.groupby('persona').agg(agg_dict).round(4)\n",
    "\n",
    "print(\"\\nüìä RESULTS BY PERSONA\\n\")\n",
    "print(\"=\"*80)\n",
    "print(persona_stats)\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "for persona in df_results['persona'].unique():\n",
    "    persona_df = df_results[df_results['persona'] == persona]\n",
    "    avg_rouge = persona_df['rougeLsum_f'].mean()\n",
    "    avg_style = persona_df['style_sim'].mean()\n",
    "    count = len(persona_df)\n",
    "    \n",
    "    print(f\"\\nüé≠ {persona} ({count} articles):\")\n",
    "    print(f\"  Content quality (ROUGE-Lsum): {avg_rouge:.4f} - {'Excellent' if avg_rouge > 0.7 else 'Good' if avg_rouge > 0.6 else 'Fair'}\")\n",
    "    print(f\"  Style fidelity: {avg_style:.4f} - {'Strong match' if avg_style > 0.75 else 'Good match' if avg_style > 0.65 else 'Moderate match'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Style Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    sns.set_style('whitegrid')\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # ROUGE scores by persona\n",
    "    df_results.boxplot(column='rougeLsum_f', by='persona', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('ROUGE-Lsum F1 by Persona')\n",
    "    axes[0, 0].set_ylabel('ROUGE-Lsum F1')\n",
    "    axes[0, 0].set_xlabel('Persona')\n",
    "    \n",
    "    # Style similarity by persona\n",
    "    df_results.boxplot(column='style_sim', by='persona', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Style Similarity by Persona')\n",
    "    axes[0, 1].set_ylabel('Style Similarity')\n",
    "    axes[0, 1].set_xlabel('Persona')\n",
    "    \n",
    "    # Scatter: ROUGE vs Style\n",
    "    for persona in df_results['persona'].unique():\n",
    "        persona_df = df_results[df_results['persona'] == persona]\n",
    "        axes[1, 0].scatter(persona_df['rougeLsum_f'], persona_df['style_sim'], \n",
    "                          label=persona, alpha=0.7, s=100)\n",
    "    axes[1, 0].set_xlabel('ROUGE-Lsum F1 (Content Quality)')\n",
    "    axes[1, 0].set_ylabel('Style Similarity')\n",
    "    axes[1, 0].set_title('Content Quality vs Style Fidelity')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # BLEURT or compression ratio\n",
    "    if bleurt_available and df_results['bleurt'].notna().any():\n",
    "        df_results.boxplot(column='bleurt', by='persona', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('BLEURT Score by Persona')\n",
    "        axes[1, 1].set_ylabel('BLEURT Score')\n",
    "    else:\n",
    "        df_results.boxplot(column='compression', by='persona', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Compression Ratio by Persona')\n",
    "        axes[1, 1].set_ylabel('Compression Ratio')\n",
    "    axes[1, 1].set_xlabel('Persona')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/evaluation_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\n‚úì Visualizations saved to outputs/evaluation_visualization.png\")\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö† Matplotlib not available for visualization\")\n",
    "    print(\"Install with: pip install matplotlib seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 8. Summary & Insights\n",
    "\n",
    "### What We Measured\n",
    "\n",
    "#### Content Quality Metrics:\n",
    "1. **ROUGE-1, ROUGE-2, ROUGE-Lsum** - N-gram overlap\n",
    "2. **BERTScore** - Semantic similarity using contextual embeddings\n",
    "3. **BLEURT** - Learned metric trained on human judgments\n",
    "\n",
    "#### Style Fidelity Metrics:\n",
    "1. **Stylometric features** - Writing style characteristics\n",
    "2. **Persona similarity** - Distance to persona centroids\n",
    "3. **Jensen-Shannon divergence** - Feature distribution similarity\n",
    "\n",
    "### Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ KEY FINDINGS\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall quality\n",
    "avg_content = df_results['rougeLsum_f'].mean()\n",
    "avg_style = df_results['style_sim'].mean()\n",
    "\n",
    "print(f\"\\n1. Overall Performance:\")\n",
    "print(f\"   - Average content quality (ROUGE-Lsum): {avg_content:.4f}\")\n",
    "if bleurt_available and df_results['bleurt'].notna().any():\n",
    "    avg_bleurt = df_results['bleurt'].mean()\n",
    "    print(f\"   - Average BLEURT score: {avg_bleurt:.4f}\")\n",
    "print(f\"   - Average style fidelity: {avg_style:.4f}\")\n",
    "print(f\"   - Assessment: {'Excellent' if avg_content > 0.65 and avg_style > 0.7 else 'Good' if avg_content > 0.55 else 'Needs improvement'}\")\n",
    "\n",
    "# Best and worst\n",
    "best_content = df_results.loc[df_results['rougeLsum_f'].idxmax()]\n",
    "worst_content = df_results.loc[df_results['rougeLsum_f'].idxmin()]\n",
    "\n",
    "print(f\"\\n2. Content Quality Range:\")\n",
    "print(f\"   - Best: {best_content['article']} (ROUGE-L: {best_content['rougeLsum_f']:.4f})\")\n",
    "print(f\"   - Worst: {worst_content['article']} (ROUGE-L: {worst_content['rougeLsum_f']:.4f})\")\n",
    "print(f\"   - Range: {df_results['rougeLsum_f'].max() - df_results['rougeLsum_f'].min():.4f}\")\n",
    "\n",
    "# Persona analysis\n",
    "print(f\"\\n3. Persona-Specific Performance:\")\n",
    "for persona in sorted(df_results['persona'].unique()):\n",
    "    persona_df = df_results[df_results['persona'] == persona]\n",
    "    print(f\"   - {persona}: content={persona_df['rougeLsum_f'].mean():.4f}, style={persona_df['style_sim'].mean():.4f}\")\n",
    "\n",
    "# Correlation\n",
    "correlation = df_results['rougeLsum_f'].corr(df_results['style_sim'])\n",
    "print(f\"\\n4. Content-Style Correlation: {correlation:.4f}\")\n",
    "print(f\"   {'Positive correlation' if correlation > 0 else 'Negative correlation'} between content quality and style match\")\n",
    "\n",
    "# Sectors\n",
    "sector_performance = df_results.groupby('sector')['rougeLsum_f'].mean().sort_values(ascending=False)\n",
    "print(f\"\\n5. Top Performing Sectors:\")\n",
    "for sector, score in sector_performance.head(3).items():\n",
    "    print(f\"   - {sector}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüí° RECOMMENDATIONS\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if avg_content < 0.6:\n",
    "    print(\"\\n‚ö† Content Quality:\")\n",
    "    print(\"  - Consider improving factual coverage in summaries\")\n",
    "    print(\"  - Ensure all key points from source are included\")\n",
    "    print(\"  - Check for information loss during summarization\")\n",
    "else:\n",
    "    print(\"\\n‚úì Content Quality: Good - summaries accurately capture key information\")\n",
    "\n",
    "if avg_style < 0.65:\n",
    "    print(\"\\n‚ö† Style Fidelity:\")\n",
    "    print(\"  - Strengthen persona-specific training examples\")\n",
    "    print(\"  - Add more diverse samples to persona corpora\")\n",
    "    print(\"  - Review persona assignment accuracy\")\n",
    "else:\n",
    "    print(\"\\n‚úì Style Fidelity: Good - summaries match target personas well\")\n",
    "\n",
    "# Check for consistency\n",
    "if df_results['rougeLsum_f'].std() > 0.15:\n",
    "    print(\"\\n‚ö† Consistency:\")\n",
    "    print(\"  - High variance in content quality across articles\")\n",
    "    print(\"  - Consider investigating low-performing examples\")\n",
    "    print(f\"  - Standard deviation: {df_results['rougeLsum_f'].std():.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚úì Consistency: Good - performance is stable across articles\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation complete! Use these insights to improve your summarization system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Run full evaluation** with all metrics:\n",
    "   ```bash\n",
    "   python -m src.eval_runner\n",
    "   ```\n",
    "\n",
    "2. **Generate detailed report**:\n",
    "   ```bash\n",
    "   python -m src.report\n",
    "   ```\n",
    "\n",
    "3. **Analyze results**:\n",
    "   - Review `outputs/per_item_metrics.csv`\n",
    "   - Check `outputs/corpus_aggregates.json`\n",
    "   - Read `outputs/report.md`\n",
    "\n",
    "4. **Iterate**:\n",
    "   - Identify low-scoring articles\n",
    "   - Analyze failure modes\n",
    "   - Improve summarization prompts/models\n",
    "   - Re-evaluate\n",
    "\n",
    "## Metrics Summary\n",
    "\n",
    "| Metric | What it measures | Interpretation |\n",
    "|--------|------------------|----------------|\n",
    "| **ROUGE-1** | Word overlap | 0.6+ = good word coverage |\n",
    "| **ROUGE-2** | Phrase overlap | 0.4+ = good phrase preservation |\n",
    "| **ROUGE-Lsum** | Structure similarity | 0.6+ = good overall quality |\n",
    "| **BERTScore** | Semantic similarity | 0.85+ = excellent, captures paraphrasing |\n",
    "| **BLEURT** | Human-like quality | 0.4+ = good, 0.6+ = excellent |\n",
    "| **Style Similarity** | Persona matching | 0.7+ = strong match, 0.65+ = good |\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've completed the evaluation demo and understand how each metric works!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}