{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Summary Evaluation Demo\n\nThis notebook demonstrates how to evaluate financial/business summaries using ROUGE, BERTScore, and stylometric analysis.\n\n## Local RoBERTa Model Setup (Optional)\n\nFor faster evaluation and offline usage, download RoBERTa-large model locally:\n\n```bash\n# Run once to download model files (~1.4GB)\npython setup_roberta.py\n\n# Verify it works\npython test_local_roberta.py\n```\n\nThe notebook will automatically use local model files if `roberta-large/` directory exists.\n\n## Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.io_utils import load_all_records\n",
    "from src.content_metrics import ContentMetricsCalculator\n",
    "from src.style_features import StyleAnalyzer\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load JSON files from data/ directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all JSON files\n",
    "records = load_all_records('data')\n",
    "\n",
    "print(f\"âœ“ Loaded {len(records)} records\")\n",
    "print(f\"\\nPersonas: {set(r['persona'] for r in records)}\")\n",
    "print(f\"Sectors: {set(r['sector'] for r in records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine One Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at first record\n",
    "example = records[0]\n",
    "\n",
    "print(\"ðŸ“° Title:\", example['document_title'])\n",
    "print(\"ðŸ“ Sector:\", example['sector'])\n",
    "print(\"ðŸ‘¤ Author:\", example['author'])\n",
    "print(\"ðŸŽ­ Persona (auto-detected):\", example['persona'])\n",
    "print(\"ðŸ¤– Model:\", example['model_used'])\n",
    "print(\"ðŸ“‹ Prompt:\", example['prompt_type'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nðŸ“„ SOURCE (first 300 chars):\")\n",
    "print(example['document_content'][:300] + \"...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nðŸŽ¯ EXPECTED SUMMARY:\")\n",
    "print(example['expected_summary'][:500] + \"...\" if len(example['expected_summary']) > 500 else example['expected_summary'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nðŸ¤– GENERATED SUMMARY:\")\n",
    "print(example['generated_summary'][:500] + \"...\" if len(example['generated_summary']) > 500 else example['generated_summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Metrics\n",
    "\n",
    "ROUGE measures n-gram overlap:\n",
    "- **ROUGE-1**: Word overlap\n",
    "- **ROUGE-2**: 2-word phrase overlap  \n",
    "- **ROUGE-L**: Longest common subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "\n",
    "reference = example['expected_summary']\n",
    "generated = example['generated_summary']\n",
    "\n",
    "scores = scorer.score(reference, generated)\n",
    "\n",
    "print(\"ðŸ“Š ROUGE Scores for Example 1:\\n\")\n",
    "for metric_name, metric_scores in scores.items():\n",
    "    print(f\"{metric_name.upper()}:\")\n",
    "    print(f\"  Precision: {metric_scores.precision:.4f}\")\n",
    "    print(f\"  Recall:    {metric_scores.recall:.4f}\")\n",
    "    print(f\"  F1:        {metric_scores.fmeasure:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTScore\n",
    "\n",
    "BERTScore uses neural embeddings to measure semantic similarity:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Local Model Check\n\nCheck if RoBERTa-large model is available locally:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nfrom pathlib import Path\n\n# Check if local RoBERTa model exists\nlocal_model_path = Path(\"roberta-large\")\n\nif local_model_path.exists() and (local_model_path / \"config.json\").exists():\n    # Set HF_HOME to use local model files\n    os.environ['HF_HOME'] = str(local_model_path.parent.absolute())\n    print(f\"âœ“ Using local RoBERTa-large model from: {local_model_path.absolute()}\")\n    print(\"  This will speed up BERTScore evaluation and works offline.\")\nelse:\n    print(\"â„¹ Local RoBERTa model not found\")\n    print(\"  BERTScore will download from HuggingFace on first use.\")\n    print(\"  To use local model: run 'python setup_roberta.py'\")\nprint()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import bert_score\n",
    "    \n",
    "    print(\"Calculating BERTScore (may take a minute on first run)...\\n\")\n",
    "    \n",
    "    P, R, F1 = bert_score.score(\n",
    "        [example['generated_summary']], \n",
    "        [example['expected_summary']],\n",
    "        model_type='roberta-large',\n",
    "        lang='en',\n",
    "        rescale_with_baseline=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸ“Š BERTScore:\")\n",
    "    print(f\"  Precision: {P.item():.4f}\")\n",
    "    print(f\"  Recall:    {R.item():.4f}\")\n",
    "    print(f\"  F1:        {F1.item():.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ BERTScore F1 of {F1.item():.4f} indicates \", end=\"\")\n",
    "    if F1.item() > 0.9:\n",
    "        print(\"excellent semantic similarity\")\n",
    "    elif F1.item() > 0.85:\n",
    "        print(\"good semantic similarity\")\n",
    "    else:\n",
    "        print(\"fair semantic similarity\")\n",
    "    \n",
    "    bertscore_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš  BERTScore demo skipped: {e}\")\n",
    "    print(\"Install with: pip install bert-score\")\n",
    "    bertscore_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stylometric Analysis\n",
    "\n",
    "Extract writing style features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "persona_config = {\n    'personas': {\n        'formal_analyst': 'data/personas/formal_analyst.txt',\n        'journalist': 'data/personas/journalist.txt',\n        'enthusiast': 'data/personas/enthusiast.txt'\n    }\n}\nstyle_analyzer = StyleAnalyzer(persona_config)\n\n# Extract features\nfeatures = style_analyzer.extract_stylometric_features(example['generated_summary'])\n\nfeature_names = [\n    'Function word rate',\n    'Avg sentence length (norm)',\n    'Type-token ratio',\n    'Comma rate',\n    'Period rate',\n    'Exclamation rate',\n    'Question rate',\n    'Pronoun rate',\n    'FK grade (norm)',\n    'Avg word length (norm)'\n]\n\nprint(\"ðŸ“ Stylometric Features:\\n\")\nfor name, value in zip(feature_names, features):\n    print(f\"{name:30s}: {value:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build persona centroids\n",
    "print(\"Building persona centroids...\\n\")\n",
    "centroids = style_analyzer.build_persona_centroids(force_rebuild=True)\n",
    "print(f\"âœ“ Built centroids for {len(centroids)} personas\\n\")\n",
    "\n",
    "# Calculate similarity to each persona\n",
    "print(f\"Summary persona: {example['persona']}\")\n",
    "print(f\"\\nðŸ“Š Style Similarity to Each Persona:\\n\")\n",
    "\n",
    "similarities = {}\n",
    "for persona_id in centroids.keys():\n",
    "    similarity = style_analyzer.calculate_style_similarity(example['generated_summary'], persona_id)\n",
    "    similarities[persona_id] = similarity\n",
    "    marker = \"âœ“\" if persona_id == example['persona'] else \" \"\n",
    "    print(f\"  {marker} {persona_id:20s}: {similarity:.4f}\")\n",
    "\n",
    "best_match = max(similarities, key=similarities.get)\n",
    "print(f\"\\nðŸ† Best match: {best_match}\")\n",
    "if best_match == example['persona']:\n",
    "    print(\"âœ“ Correct!\")\n",
    "else:\n",
    "    print(f\"âœ— Mismatch (expected {example['persona']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Evaluation\n",
    "\n",
    "Run full evaluation on all records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results = []\n\nprint(f\"Evaluating {len(records)} records...\\n\")\n\ncontent_config = {\n    'use_rouge': True,\n    'use_bertscore': bertscore_available,\n    'bertscore_model': 'roberta-large',\n    'use_bleurt': False\n}\n\ncontent_calc = ContentMetricsCalculator({'content': content_config})\n\n# Re-use the style_analyzer from earlier (already has persona config)\n# Or create a new one with same config\nif 'style_analyzer' not in dir():\n    persona_config = {\n        'personas': {\n            'formal_analyst': 'data/personas/formal_analyst.txt',\n            'journalist': 'data/personas/journalist.txt',\n            'enthusiast': 'data/personas/enthusiast.txt'\n        }\n    }\n    style_analyzer = StyleAnalyzer(persona_config)\n\nfor i, rec in enumerate(records, 1):\n    print(f\"  [{i}/{len(records)}] {rec['document_title'][:50]}...\")\n    \n    # Calculate content metrics\n    content_metrics = content_calc.calculate_all_metrics(\n        source_text=rec['document_content'],\n        reference=rec['expected_summary'],\n        hypothesis=rec['generated_summary']\n    )\n    \n    # Calculate style similarity\n    style_sim = style_analyzer.calculate_style_similarity(\n        rec['generated_summary'], \n        rec['persona']\n    )\n    \n    # Calculate overall quality (70% content + 30% style)\n    content_quality = content_metrics.get('content_quality', 0.0)\n    style_score = style_sim if style_sim is not None else 0.0\n    overall_quality = (0.7 * content_quality) + (0.3 * style_score)\n    \n    results.append({\n        'file': rec['_source_file'],\n        'title': rec['document_title'][:40],\n        'sector': rec['sector'],\n        'persona': rec['persona'],\n        'model': rec['model_used'],\n        **content_metrics,\n        'style_similarity': style_sim,\n        'overall_quality': overall_quality\n    })\n\ndf = pd.DataFrame(results)\nprint(\"\\nâœ“ Evaluation complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nðŸ“Š EVALUATION RESULTS\\n\")\nprint(\"=\"*80)\n\n# Display key columns\ndisplay_cols = ['title', 'persona', 'overall_quality', 'rouge1_f', 'rougeLsum_f']\nif bertscore_available and 'bertscore_f1' in df.columns:\n    display_cols.append('bertscore_f1')\ndisplay_cols.append('style_similarity')\n\nprint(df[display_cols].to_string(index=False))\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nðŸ“ˆ SUMMARY STATISTICS\\n\")\n\nprint(\"Overall Quality (70% content + 30% style):\")\nprint(f\"  Overall Quality:  {df['overall_quality'].mean():.4f} Â± {df['overall_quality'].std():.4f}\")\nprint(f\"  Min: {df['overall_quality'].min():.4f}, Max: {df['overall_quality'].max():.4f}\")\n\nprint(f\"\\nContent Quality:\")\nprint(f\"  Content Quality:  {df['content_quality'].mean():.4f} Â± {df['content_quality'].std():.4f}\")\nprint(f\"  ROUGE-1 F1:       {df['rouge1_f'].mean():.4f} Â± {df['rouge1_f'].std():.4f}\")\nprint(f\"  ROUGE-Lsum F1:    {df['rougeLsum_f'].mean():.4f} Â± {df['rougeLsum_f'].std():.4f}\")\n\nif bertscore_available and 'bertscore_f1' in df.columns:\n    print(f\"  BERTScore F1:     {df['bertscore_f1'].mean():.4f} Â± {df['bertscore_f1'].std():.4f}\")\n\nprint(f\"\\nStyle Fidelity:\")\nprint(f\"  Style Similarity: {df['style_similarity'].mean():.4f} Â± {df['style_similarity'].std():.4f}\")\n\nprint(f\"\\nBy Persona:\")\nfor persona in sorted(df['persona'].unique()):\n    persona_df = df[df['persona'] == persona]\n    print(f\"  {persona:20s}: {len(persona_df)} items\")\n    print(f\"    Overall:  {persona_df['overall_quality'].mean():.3f}\")\n    print(f\"    Content:  {persona_df['content_quality'].mean():.3f}\")\n    print(f\"    ROUGE-L:  {persona_df['rougeLsum_f'].mean():.3f}\")\n    print(f\"    Style:    {persona_df['style_similarity'].mean():.3f}\")\n\nprint(f\"\\nBy Sector:\")\nfor sector in sorted(df['sector'].unique()):\n    sector_df = df[df['sector'] == sector]\n    print(f\"  {sector:20s}: {len(sector_df)} items, overall={sector_df['overall_quality'].mean():.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "try:\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    \n    sns.set_style('whitegrid')\n    \n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Overall Quality vs ROUGE\n    for persona in df['persona'].unique():\n        persona_df = df[df['persona'] == persona]\n        axes[0, 0].scatter(persona_df['rougeLsum_f'], persona_df['overall_quality'], \n                       label=persona, alpha=0.7, s=150)\n    axes[0, 0].set_xlabel('ROUGE-Lsum F1')\n    axes[0, 0].set_ylabel('Overall Quality')\n    axes[0, 0].set_title('ROUGE vs Overall Quality')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Content vs Style\n    for persona in df['persona'].unique():\n        persona_df = df[df['persona'] == persona]\n        axes[0, 1].scatter(persona_df['content_quality'], persona_df['style_similarity'], \n                       label=persona, alpha=0.7, s=150)\n    axes[0, 1].set_xlabel('Content Quality')\n    axes[0, 1].set_ylabel('Style Similarity')\n    axes[0, 1].set_title('Content Quality vs Style Fidelity')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Overall quality by persona\n    df.boxplot(column='overall_quality', by='persona', ax=axes[1, 0])\n    axes[1, 0].set_title('Overall Quality by Persona')\n    axes[1, 0].set_ylabel('Overall Quality')\n    axes[1, 0].set_xlabel('Persona')\n    \n    # Overall quality by sector\n    df.boxplot(column='overall_quality', by='sector', ax=axes[1, 1])\n    axes[1, 1].set_title('Overall Quality by Sector')\n    axes[1, 1].set_ylabel('Overall Quality')\n    axes[1, 1].set_xlabel('Sector')\n    plt.xticks(rotation=45)\n    \n    plt.tight_layout()\n    plt.savefig('outputs/evaluation_viz.png', dpi=150, bbox_inches='tight')\n    print(\"\\nâœ“ Saved visualization to outputs/evaluation_viz.png\")\n    plt.show()\n    \nexcept ImportError:\n    print(\"âš  Matplotlib not available\")\n    print(\"Install with: pip install matplotlib seaborn\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\n1. **Add more JSON files** to `data/` directory\n2. **Run full evaluation**: `python -m src.eval_runner`\n3. **Check results**: `outputs/per_item_metrics.csv`\n4. **Customize personas**: Edit `data/personas/*.txt`\n\n### Interpreting Scores\n\n**Overall Quality (Combined Metric):**\n- Overall Quality > 0.6: Good summary (70% content + 30% style)\n- Overall Quality > 0.7: Excellent summary\n\n**Content Quality:**\n- Content Quality > 0.6: Good content coverage (weighted ROUGE + BERTScore)\n- ROUGE-Lsum F1 > 0.6: Good overlap with reference\n- BERTScore F1 > 0.85: Strong semantic match\n\n**Style Fidelity:**\n- Style similarity > 0.7: Good persona match\n- Style similarity > 0.8: Strong persona match\n\n### Metric Weighting\n\nThe overall quality metric combines:\n- **70% Content Quality**: Weighted average of ROUGE (40%), BERTScore (30%), BLEURT (30%)\n- **30% Style Similarity**: Jensen-Shannon divergence of stylometric features"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}