{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple BERTScore Test - Local RoBERTa Model\n",
    "\n",
    "This notebook loads RoBERTa-large directly from a flat directory with 4 files:\n",
    "- config.json\n",
    "- merges.txt  \n",
    "- pytorch_model.bin\n",
    "- vocab.json\n",
    "\n",
    "No cache, no complex structure. Just works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nfrom pathlib import Path\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\nprint(\"âœ“ Imports successful\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the model directory exists with required files\n",
    "model_dir = Path(\"roberta-large\")\n",
    "\n",
    "required_files = [\"config.json\", \"merges.txt\", \"pytorch_model.bin\", \"vocab.json\"]\n",
    "\n",
    "print(f\"Checking directory: {model_dir.absolute()}\")\n",
    "print()\n",
    "\n",
    "for file in required_files:\n",
    "    file_path = model_dir / file\n",
    "    if file_path.exists():\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"âœ“ {file:25s} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"âœ— {file:25s} MISSING\")\n",
    "        raise FileNotFoundError(f\"Required file missing: {file}\")\n",
    "\n",
    "print(\"\\nâœ“ All required files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer directly from local directory\n",
    "print(\"Loading tokenizer from local directory...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    str(model_dir),\n",
    "    local_files_only=True  # Don't try to download anything\n",
    ")\n",
    "print(\"âœ“ Tokenizer loaded\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly from local directory\n",
    "print(\"Loading RoBERTa model from local directory...\")\n",
    "print(\"(This may take 10-30 seconds)\")\n",
    "model = AutoModel.from_pretrained(\n",
    "    str(model_dir),\n",
    "    local_files_only=True  # Don't try to download anything\n",
    ")\n",
    "print(\"âœ“ Model loaded\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample text\n",
    "test_text = \"The company reported strong quarterly earnings with revenue growth of 15%.\"\n",
    "\n",
    "print(\"Testing tokenization...\")\n",
    "tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"âœ“ Tokenized: {len(tokens['input_ids'][0])} tokens\")\n",
    "print(f\"  Tokens: {tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][:10])}...\")\n",
    "\n",
    "print(\"\\nTesting model inference...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    \n",
    "print(f\"âœ“ Model inference successful\")\n",
    "print(f\"  Output shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"  Embedding dim: {outputs.last_hidden_state.shape[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## BERTScore Calculation\n\nNow let's compute BERTScore manually using our local model:\n\nWe'll compute it ourselves instead of using bert_score library's score() function, which doesn't accept model parameters in older versions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute BERTScore manually using our local model\n# (bert_score library doesn't accept model parameter in older versions)\n\ndef compute_bertscore(reference, candidate, model, tokenizer):\n    \"\"\"\n    Compute BERTScore manually using local model.\n    \n    Returns: (precision, recall, f1)\n    \"\"\"\n    # Tokenize\n    ref_tokens = tokenizer(reference, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    cand_tokens = tokenizer(candidate, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    \n    # Get embeddings\n    with torch.no_grad():\n        ref_outputs = model(**ref_tokens)\n        cand_outputs = model(**cand_tokens)\n    \n    # Get last hidden states (remove padding tokens)\n    ref_embeds = ref_outputs.last_hidden_state[0]  # [seq_len, hidden_dim]\n    cand_embeds = cand_outputs.last_hidden_state[0]\n    \n    # Remove CLS and SEP tokens (first and last)\n    ref_embeds = ref_embeds[1:-1]\n    cand_embeds = cand_embeds[1:-1]\n    \n    # Normalize embeddings\n    ref_embeds = F.normalize(ref_embeds, p=2, dim=1)\n    cand_embeds = F.normalize(cand_embeds, p=2, dim=1)\n    \n    # Compute cosine similarity matrix\n    sim_matrix = torch.mm(cand_embeds, ref_embeds.t())  # [cand_len, ref_len]\n    \n    # Precision: for each candidate token, find max similarity with reference\n    precision = sim_matrix.max(dim=1)[0].mean().item()\n    \n    # Recall: for each reference token, find max similarity with candidate\n    recall = sim_matrix.max(dim=0)[0].mean().item()\n    \n    # F1\n    if precision + recall > 0:\n        f1 = 2 * (precision * recall) / (precision + recall)\n    else:\n        f1 = 0.0\n    \n    return precision, recall, f1\n\n\n# Test sentences\nreference = \"The company reported strong quarterly earnings with revenue growth of 15%.\"\ncandidate = \"Quarterly results showed solid performance, with revenues up 15%.\"\n\nprint(\"Computing BERTScore manually with local model...\")\nprint(f\"Reference: {reference}\")\nprint(f\"Candidate: {candidate}\")\nprint()\n\nP, R, F1 = compute_bertscore(reference, candidate, model, tokenizer)\n\nprint(\"âœ“ BERTScore calculated successfully!\")\nprint()\nprint(f\"Precision: {P:.4f}\")\nprint(f\"Recall:    {R:.4f}\")\nprint(f\"F1 Score:  {F1:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with multiple examples using our manual BERTScore function\ntest_cases = [\n    {\n        \"ref\": \"Tesla reported record deliveries in Q4 2024, exceeding analyst expectations.\",\n        \"cand\": \"Tesla's Q4 2024 deliveries surpassed predictions from analysts.\"\n    },\n    {\n        \"ref\": \"The Federal Reserve maintained interest rates at current levels.\",\n        \"cand\": \"Fed kept rates unchanged at today's meeting.\"\n    },\n    {\n        \"ref\": \"Amazon Web Services announced new AI infrastructure capabilities.\",\n        \"cand\": \"AWS unveiled enhanced artificial intelligence infrastructure.\"\n    }\n]\n\nprint(\"Testing multiple sentence pairs:\\n\")\nprint(\"=\"*80)\n\nfor i, case in enumerate(test_cases, 1):\n    P, R, F1 = compute_bertscore(case['ref'], case['cand'], model, tokenizer)\n    \n    print(f\"\\nExample {i}:\")\n    print(f\"  Ref:  {case['ref'][:60]}...\" if len(case['ref']) > 60 else f\"  Ref:  {case['ref']}\")\n    print(f\"  Cand: {case['cand'][:60]}...\" if len(case['cand']) > 60 else f\"  Cand: {case['cand']}\")\n    print(f\"  â†’ BERTScore F1: {F1:.4f} (P: {P:.4f}, R: {R:.4f})\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\\nâœ“ All tests completed successfully!\")\nprint(\"\\nðŸ’¡ Your local RoBERTa model is working perfectly!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrates that you can use RoBERTa-large for BERTScore with just 4 files in a flat directory:\n\n1. âœ… Load tokenizer from local directory with `local_files_only=True`\n2. âœ… Load model from local directory with `local_files_only=True`\n3. âœ… Compute BERTScore manually using the local model\n4. âœ… No caching, no complex directory structure needed\n5. âœ… No internet required after initial setup\n\n**Key Steps:**\n```python\n# Load from local flat directory\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-large\", local_files_only=True)\nmodel = AutoModel.from_pretrained(\"roberta-large\", local_files_only=True)\n\n# Compute BERTScore manually\ndef compute_bertscore(reference, candidate, model, tokenizer):\n    # Tokenize both texts\n    ref_tokens = tokenizer(reference, return_tensors=\"pt\")\n    cand_tokens = tokenizer(candidate, return_tensors=\"pt\")\n    \n    # Get embeddings from model\n    with torch.no_grad():\n        ref_embeds = model(**ref_tokens).last_hidden_state[0]\n        cand_embeds = model(**cand_tokens).last_hidden_state[0]\n    \n    # Normalize and compute similarity\n    ref_embeds = F.normalize(ref_embeds[1:-1], p=2, dim=1)  # Remove CLS/SEP\n    cand_embeds = F.normalize(cand_embeds[1:-1], p=2, dim=1)\n    \n    # Cosine similarity matrix\n    sim_matrix = torch.mm(cand_embeds, ref_embeds.t())\n    \n    # Greedy matching\n    precision = sim_matrix.max(dim=1)[0].mean().item()\n    recall = sim_matrix.max(dim=0)[0].mean().item()\n    f1 = 2 * (precision * recall) / (precision + recall)\n    \n    return precision, recall, f1\n```\n\n**No dependency on bert_score library's score() function** - we compute it ourselves using just transformers and torch!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}