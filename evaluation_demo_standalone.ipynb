{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Summary Evaluation Demo (Standalone Version)\n\nThis notebook demonstrates how to evaluate financial/business summaries using ROUGE, BERTScore, and stylometric analysis.\n\n**This is a standalone version with all code embedded - no external src/ dependencies required.**\n\n## Local RoBERTa Model Setup (Optional)\n\nFor faster BERTScore evaluation and offline usage, download RoBERTa-large model locally:\n\n```bash\n# Run once to download model files (~1.4GB)\npython setup_roberta.py\n\n# Verify it works\npython test_local_roberta.py\n```\n\nThe notebook will automatically detect and use local model files if `roberta-large/` directory exists.\n\n## Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Dict, Any, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from rouge_score import rouge_scorer\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import nltk\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Utilities\n",
    "\n",
    "Basic text processing functions for tokenization and stylometric analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_nltk_data():\n",
    "    \"\"\"Download required NLTK data if not present.\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    \n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt_tab')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "\n",
    "def tokenize_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Tokenize text into sentences.\"\"\"\n",
    "    ensure_nltk_data()\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "\n",
    "def tokenize_words(text: str) -> List[str]:\n",
    "    \"\"\"Tokenize text into words.\"\"\"\n",
    "    ensure_nltk_data()\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Count number of tokens in text.\"\"\"\n",
    "    return len(tokenize_words(text))\n",
    "\n",
    "\n",
    "def get_function_words() -> set:\n",
    "    \"\"\"Get set of common function words for stylometric analysis.\"\"\"\n",
    "    return {\n",
    "        'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i',\n",
    "        'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at',\n",
    "        'this', 'but', 'his', 'by', 'from', 'they', 'we', 'say', 'her', 'she',\n",
    "        'or', 'an', 'will', 'my', 'one', 'all', 'would', 'there', 'their',\n",
    "        'what', 'so', 'up', 'out', 'if', 'about', 'who', 'get', 'which', 'go',\n",
    "        'me', 'when', 'make', 'can', 'like', 'time', 'no', 'just', 'him', 'know',\n",
    "        'take', 'people', 'into', 'year', 'your', 'good', 'some', 'could', 'them',\n",
    "        'see', 'other', 'than', 'then', 'now', 'look', 'only', 'come', 'its', 'over'\n",
    "    }\n",
    "\n",
    "\n",
    "def get_pronouns() -> set:\n",
    "    \"\"\"Get set of pronouns for stylometric analysis.\"\"\"\n",
    "    return {\n",
    "        'i', 'you', 'he', 'she', 'it', 'we', 'they',\n",
    "        'me', 'him', 'her', 'us', 'them',\n",
    "        'my', 'your', 'his', 'her', 'its', 'our', 'their',\n",
    "        'mine', 'yours', 'hers', 'ours', 'theirs',\n",
    "        'myself', 'yourself', 'himself', 'herself', 'itself', 'ourselves', 'themselves'\n",
    "    }\n",
    "\n",
    "\n",
    "def _count_syllables(word: str) -> int:\n",
    "    \"\"\"Count syllables in a word (approximation).\"\"\"\n",
    "    word = word.lower()\n",
    "    vowels = 'aeiouy'\n",
    "    syllable_count = 0\n",
    "    previous_was_vowel = False\n",
    "    \n",
    "    for char in word:\n",
    "        is_vowel = char in vowels\n",
    "        if is_vowel and not previous_was_vowel:\n",
    "            syllable_count += 1\n",
    "        previous_was_vowel = is_vowel\n",
    "    \n",
    "    # Adjust for silent 'e'\n",
    "    if word.endswith('e'):\n",
    "        syllable_count -= 1\n",
    "    \n",
    "    # Ensure at least 1 syllable\n",
    "    return max(1, syllable_count)\n",
    "\n",
    "\n",
    "def calculate_flesch_kincaid_grade(text: str) -> float:\n",
    "    \"\"\"Calculate Flesch-Kincaid grade level.\"\"\"\n",
    "    sentences = tokenize_sentences(text)\n",
    "    words = tokenize_words(text)\n",
    "    \n",
    "    if not sentences or not words:\n",
    "        return 0.0\n",
    "    \n",
    "    # Count syllables (approximation)\n",
    "    syllable_count = sum(_count_syllables(word) for word in words)\n",
    "    \n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(words)\n",
    "    \n",
    "    if num_sentences == 0 or num_words == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # FK grade formula\n",
    "    grade = 0.39 * (num_words / num_sentences) + 11.8 * (syllable_count / num_words) - 15.59\n",
    "    \n",
    "    return max(0.0, grade)\n",
    "\n",
    "\n",
    "def get_punctuation_counts(text: str) -> dict:\n",
    "    \"\"\"Count different types of punctuation.\"\"\"\n",
    "    return {\n",
    "        'comma': text.count(','),\n",
    "        'period': text.count('.'),\n",
    "        'semicolon': text.count(';'),\n",
    "        'colon': text.count(':'),\n",
    "        'exclamation': text.count('!'),\n",
    "        'question': text.count('?'),\n",
    "        'total': sum(1 for c in text if c in string.punctuation)\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Text utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O Utilities\n",
    "\n",
    "Functions for loading and processing JSON data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_files(data_dir: str = \"data\") -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"Load all JSON files from a directory.\"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    json_files = sorted(data_path.glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        raise FileNotFoundError(f\"No JSON files found in {data_dir}\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                # Add filename for tracking\n",
    "                data['_source_file'] = json_file.name\n",
    "                yield data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: Failed to parse {json_file.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "def extract_fields(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Extract all relevant fields from a record.\"\"\"\n",
    "    # Extract metadata\n",
    "    metadata = record.get('metadata', {})\n",
    "    \n",
    "    return {\n",
    "        # Core content\n",
    "        'document_content': record.get('document_content', ''),\n",
    "        'expected_summary': record.get('expected_summary', ''),\n",
    "        'generated_summary': record.get('generated_summary', ''),\n",
    "        \n",
    "        # Metadata\n",
    "        'document_title': record.get('document_title', ''),\n",
    "        'link': record.get('link', ''),\n",
    "        'author': metadata.get('author', ''),\n",
    "        'sector': metadata.get('sector', ''),\n",
    "        'region': metadata.get('region', ''),\n",
    "        'date': metadata.get('date', ''),\n",
    "        'wire_id': metadata.get('wire_id', ''),\n",
    "        'subject_codes': metadata.get('subject_codes', []),\n",
    "        \n",
    "        # Model info\n",
    "        'prompt_type': record.get('prompt_type', ''),\n",
    "        'model_used': record.get('model_used', ''),\n",
    "        'export_timestamp': record.get('export_timestamp', ''),\n",
    "        \n",
    "        # Tracking\n",
    "        '_source_file': record.get('_source_file', ''),\n",
    "    }\n",
    "\n",
    "\n",
    "def infer_persona(record: Dict[str, Any]) -> str:\n",
    "    \"\"\"Infer persona from record metadata.\"\"\"\n",
    "    prompt_type = record.get('prompt_type', '').lower()\n",
    "    author = record.get('author', '').lower()\n",
    "    sector = record.get('sector', '').lower()\n",
    "    \n",
    "    # Formal analyst style: research notes, detailed analysis\n",
    "    if any(keyword in prompt_type for keyword in ['research', 'analyst', 'note']):\n",
    "        return 'formal_analyst'\n",
    "    \n",
    "    if any(keyword in author for keyword in ['analyst', 'economist', 'strategist', 'phd', 'cfa']):\n",
    "        return 'formal_analyst'\n",
    "    \n",
    "    # Journalist style: morning summaries, news-style\n",
    "    if any(keyword in prompt_type for keyword in ['morning', 'summary', 'brief', 'update']):\n",
    "        return 'journalist'\n",
    "    \n",
    "    # Enthusiast style: quick takes, highlights\n",
    "    if any(keyword in prompt_type for keyword in ['quick', 'highlight', 'takeaway']):\n",
    "        return 'enthusiast'\n",
    "    \n",
    "    # Default to journalist for financial/business content\n",
    "    return 'journalist'\n",
    "\n",
    "\n",
    "def load_all_records(data_dir: str = \"data\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load and process all JSON files from directory.\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for raw_record in load_json_files(data_dir):\n",
    "        # Extract all fields\n",
    "        extracted = extract_fields(raw_record)\n",
    "        \n",
    "        # Infer persona\n",
    "        extracted['persona'] = infer_persona(extracted)\n",
    "        \n",
    "        records.append(extracted)\n",
    "    \n",
    "    return records\n",
    "\n",
    "print(\"âœ“ I/O utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Metrics Calculator\n",
    "\n",
    "Calculate ROUGE, BERTScore, and BLEURT metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentMetricsCalculator:\n",
    "    \"\"\"Calculate content quality metrics for summaries.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialize metrics calculator.\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dictionary with content metrics settings\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.content_config = config.get('content', {})\n",
    "        \n",
    "        # Initialize ROUGE\n",
    "        if self.content_config.get('use_rouge', True):\n",
    "            rouge_types = ['rouge1', 'rouge2', 'rougeLsum']\n",
    "            self.rouge_scorer = rouge_scorer.RougeScorer(rouge_types, use_stemmer=True)\n",
    "        else:\n",
    "            self.rouge_scorer = None\n",
    "        \n",
    "        # BERTScore model\n",
    "        self.bertscore_model = self.content_config.get('bertscore_model', 'roberta-large')\n",
    "        \n",
    "        # BLEURT checkpoint\n",
    "        self.bleurt_checkpoint = self.content_config.get('bleurt_checkpoint', 'BLEURT-20-D12')\n",
    "        self.bleurt_scorer = None  # Lazy load\n",
    "    \n",
    "    def _init_bleurt(self):\n",
    "        \"\"\"Lazy initialization of BLEURT scorer.\"\"\"\n",
    "        if self.bleurt_scorer is None and self.content_config.get('use_bleurt', True):\n",
    "            try:\n",
    "                from bleurt import score\n",
    "                self.bleurt_scorer = score.BleurtScorer(self.bleurt_checkpoint)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: BLEURT initialization failed: {e}\")\n",
    "                self.bleurt_scorer = False\n",
    "    \n",
    "    def calculate_rouge(self, reference: str, hypothesis: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate ROUGE scores.\"\"\"\n",
    "        if not self.rouge_scorer:\n",
    "            return {}\n",
    "        \n",
    "        scores = self.rouge_scorer.score(reference, hypothesis)\n",
    "        \n",
    "        return {\n",
    "            'rouge1_f': scores['rouge1'].fmeasure,\n",
    "            'rouge2_f': scores['rouge2'].fmeasure,\n",
    "            'rougeLsum_f': scores['rougeLsum'].fmeasure,\n",
    "            'rouge1_r': scores['rouge1'].recall,\n",
    "            'rouge2_r': scores['rouge2'].recall,\n",
    "            'rougeLsum_r': scores['rougeLsum'].recall,\n",
    "        }\n",
    "    \n",
    "    def calculate_bertscore(self, reference: str, hypothesis: str) -> Optional[float]:\n",
    "        \"\"\"Calculate BERTScore.\"\"\"\n",
    "        if not self.content_config.get('use_bertscore', True):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            import bert_score\n",
    "            P, R, F1 = bert_score.score(\n",
    "                [hypothesis],\n",
    "                [reference],\n",
    "                model_type=self.bertscore_model,\n",
    "                lang='en',\n",
    "                rescale_with_baseline=True,\n",
    "                verbose=False\n",
    "            )\n",
    "            return F1.item()\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: BERTScore calculation failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_bleurt(self, reference: str, hypothesis: str) -> Optional[float]:\n",
    "        \"\"\"Calculate BLEURT score.\"\"\"\n",
    "        if not self.content_config.get('use_bleurt', True):\n",
    "            return None\n",
    "        \n",
    "        self._init_bleurt()\n",
    "        \n",
    "        if not self.bleurt_scorer:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            scores = self.bleurt_scorer.score(\n",
    "                references=[reference],\n",
    "                candidates=[hypothesis]\n",
    "            )\n",
    "            return scores[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: BLEURT calculation failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def calculate_all_metrics(\n",
    "        self,\n",
    "        source_text: str,\n",
    "        reference: str,\n",
    "        hypothesis: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate all content metrics for a single example.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # ROUGE\n",
    "        rouge_scores = self.calculate_rouge(reference, hypothesis)\n",
    "        metrics.update(rouge_scores)\n",
    "        \n",
    "        # BERTScore\n",
    "        bertscore_f1 = self.calculate_bertscore(reference, hypothesis)\n",
    "        metrics['bertscore_f1'] = bertscore_f1\n",
    "        \n",
    "        # BLEURT\n",
    "        bleurt_score = self.calculate_bleurt(reference, hypothesis)\n",
    "        metrics['bleurt'] = bleurt_score\n",
    "        \n",
    "        # Token counts and compression ratio\n",
    "        src_tokens = count_tokens(source_text)\n",
    "        hyp_tokens = count_tokens(hypothesis)\n",
    "        gold_tokens = count_tokens(reference)\n",
    "        \n",
    "        metrics['src_tokens'] = src_tokens\n",
    "        metrics['hyp_tokens'] = hyp_tokens\n",
    "        metrics['gold_tokens'] = gold_tokens\n",
    "        metrics['compression_ratio'] = hyp_tokens / src_tokens if src_tokens > 0 else 0.0\n",
    "        \n",
    "        # Composite score\n",
    "        rougeLsum_f = metrics.get('rougeLsum_f', 0.0)\n",
    "        bertscore = bertscore_f1 if bertscore_f1 is not None else 0.0\n",
    "        bleurt = bleurt_score if bleurt_score is not None else 0.0\n",
    "        \n",
    "        # Normalize BLEURT to 0-1 range (BLEURT typically ranges from -1 to 1)\n",
    "        bleurt_normalized = (bleurt + 1) / 2 if bleurt is not None else 0.0\n",
    "        \n",
    "        metrics['content_quality'] = (\n",
    "            0.4 * rougeLsum_f +\n",
    "            0.3 * bertscore +\n",
    "            0.3 * bleurt_normalized\n",
    "        )\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "print(\"âœ“ ContentMetricsCalculator loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Analyzer\n",
    "\n",
    "Extract stylometric features and calculate persona similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleAnalyzer:\n",
    "    \"\"\"Analyze stylometric features and calculate persona similarity.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initialize style analyzer.\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dictionary with style settings and persona paths\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.style_config = config.get('style', {})\n",
    "        self.personas = config.get('personas', {})\n",
    "        self.centroids = {}\n",
    "        self.function_words = get_function_words()\n",
    "        self.pronouns = get_pronouns()\n",
    "    \n",
    "    def extract_stylometric_features(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Extract stylometric feature vector from text.\"\"\"\n",
    "        if not text.strip():\n",
    "            return np.zeros(10)\n",
    "        \n",
    "        # Tokenize\n",
    "        sentences = tokenize_sentences(text)\n",
    "        words = tokenize_words(text)\n",
    "        words_lower = [w.lower() for w in words]\n",
    "        \n",
    "        if not words:\n",
    "            return np.zeros(10)\n",
    "        \n",
    "        # Feature 1: Function word rate\n",
    "        function_word_count = sum(1 for w in words_lower if w in self.function_words)\n",
    "        function_word_rate = function_word_count / len(words)\n",
    "        \n",
    "        # Feature 2: Average sentence length\n",
    "        avg_sentence_length = len(words) / len(sentences) if sentences else 0\n",
    "        \n",
    "        # Feature 3: Type-token ratio (vocabulary diversity)\n",
    "        type_token_ratio = len(set(words_lower)) / len(words) if words else 0\n",
    "        \n",
    "        # Feature 4-7: Punctuation rates\n",
    "        punct_counts = get_punctuation_counts(text)\n",
    "        total_chars = len(text)\n",
    "        comma_rate = punct_counts['comma'] / total_chars if total_chars > 0 else 0\n",
    "        period_rate = punct_counts['period'] / total_chars if total_chars > 0 else 0\n",
    "        exclamation_rate = punct_counts['exclamation'] / total_chars if total_chars > 0 else 0\n",
    "        question_rate = punct_counts['question'] / total_chars if total_chars > 0 else 0\n",
    "        \n",
    "        # Feature 8: Pronoun rate\n",
    "        pronoun_count = sum(1 for w in words_lower if w in self.pronouns)\n",
    "        pronoun_rate = pronoun_count / len(words)\n",
    "        \n",
    "        # Feature 9: Flesch-Kincaid grade\n",
    "        fk_grade = calculate_flesch_kincaid_grade(text)\n",
    "        fk_grade_normalized = min(fk_grade / 20.0, 1.0)  # Normalize to 0-1\n",
    "        \n",
    "        # Feature 10: Average word length\n",
    "        avg_word_length = sum(len(w) for w in words) / len(words)\n",
    "        avg_word_length_normalized = min(avg_word_length / 10.0, 1.0)\n",
    "        \n",
    "        features = np.array([\n",
    "            function_word_rate,\n",
    "            avg_sentence_length / 50.0,  # Normalize (assume max 50 words/sentence)\n",
    "            type_token_ratio,\n",
    "            comma_rate * 100,  # Scale up\n",
    "            period_rate * 100,\n",
    "            exclamation_rate * 100,\n",
    "            question_rate * 100,\n",
    "            pronoun_rate,\n",
    "            fk_grade_normalized,\n",
    "            avg_word_length_normalized\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def build_persona_centroids(self, force_rebuild: bool = False) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Build stylometric centroids for each persona from their corpus files.\"\"\"\n",
    "        cache_path = Path('outputs/persona_centroids.json')\n",
    "        \n",
    "        # Try to load from cache\n",
    "        if not force_rebuild and cache_path.exists():\n",
    "            with open(cache_path, 'r') as f:\n",
    "                cached = json.load(f)\n",
    "                self.centroids = {k: np.array(v) for k, v in cached.items()}\n",
    "                return self.centroids\n",
    "        \n",
    "        centroids = {}\n",
    "        \n",
    "        for persona_id, corpus_path in self.personas.items():\n",
    "            if not Path(corpus_path).exists():\n",
    "                print(f\"Warning: Persona corpus not found: {corpus_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Load persona corpus\n",
    "            with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "                corpus_text = f.read()\n",
    "            \n",
    "            # Split into samples (assume samples separated by double newlines)\n",
    "            samples = [s.strip() for s in corpus_text.split('\\n\\n') if s.strip()]\n",
    "            \n",
    "            if not samples:\n",
    "                print(f\"Warning: No samples found for persona {persona_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract features from each sample\n",
    "            feature_vectors = []\n",
    "            for sample in samples:\n",
    "                features = self.extract_stylometric_features(sample)\n",
    "                feature_vectors.append(features)\n",
    "            \n",
    "            # Calculate centroid (mean of all samples)\n",
    "            centroid = np.mean(feature_vectors, axis=0)\n",
    "            centroids[persona_id] = centroid\n",
    "        \n",
    "        # Cache centroids\n",
    "        cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(cache_path, 'w') as f:\n",
    "            json.dump({k: v.tolist() for k, v in centroids.items()}, f, indent=2)\n",
    "        \n",
    "        self.centroids = centroids\n",
    "        return centroids\n",
    "    \n",
    "    def calculate_style_similarity(\n",
    "        self,\n",
    "        text: str,\n",
    "        persona_id: Optional[str]\n",
    "    ) -> Optional[float]:\n",
    "        \"\"\"Calculate stylometric similarity between text and persona centroid.\"\"\"\n",
    "        if not self.style_config.get('use_stylometric_similarity', True):\n",
    "            return None\n",
    "        \n",
    "        if persona_id is None:\n",
    "            return None\n",
    "        \n",
    "        # Ensure centroids are built\n",
    "        if not self.centroids:\n",
    "            self.build_persona_centroids()\n",
    "        \n",
    "        if persona_id not in self.centroids:\n",
    "            print(f\"Warning: No centroid for persona {persona_id}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract features from text\n",
    "        text_features = self.extract_stylometric_features(text)\n",
    "        persona_centroid = self.centroids[persona_id]\n",
    "        \n",
    "        # Normalize features to probability distributions (add small epsilon to avoid zeros)\n",
    "        epsilon = 1e-10\n",
    "        text_dist = text_features + epsilon\n",
    "        text_dist = text_dist / text_dist.sum()\n",
    "        \n",
    "        centroid_dist = persona_centroid + epsilon\n",
    "        centroid_dist = centroid_dist / centroid_dist.sum()\n",
    "        \n",
    "        # Calculate Jensen-Shannon divergence\n",
    "        js_divergence = jensenshannon(text_dist, centroid_dist)\n",
    "        \n",
    "        # Convert to similarity (1 - divergence)\n",
    "        # JS divergence is in [0, 1] for probability distributions\n",
    "        similarity = 1.0 - js_divergence\n",
    "        \n",
    "        return float(similarity)\n",
    "    \n",
    "    def calculate_style_metrics(\n",
    "        self,\n",
    "        hypothesis: str,\n",
    "        persona_id: Optional[str]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate all style metrics for a summary.\"\"\"\n",
    "        style_similarity = self.calculate_style_similarity(hypothesis, persona_id)\n",
    "        style_skipped = 1 if (persona_id is None or style_similarity is None) else 0\n",
    "        \n",
    "        return {\n",
    "            'style_similarity': style_similarity,\n",
    "            'style_skipped': style_skipped,\n",
    "            'style_fidelity': style_similarity  # Alias for composite metric\n",
    "        }\n",
    "\n",
    "print(\"âœ“ StyleAnalyzer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load JSON files from data/ directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all JSON files\n",
    "records = load_all_records('data')\n",
    "\n",
    "print(f\"âœ“ Loaded {len(records)} records\")\n",
    "print(f\"\\nPersonas: {set(r['persona'] for r in records)}\")\n",
    "print(f\"Sectors: {set(r['sector'] for r in records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine One Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at first record\n",
    "example = records[0]\n",
    "\n",
    "print(\"ðŸ“° Title:\", example['document_title'])\n",
    "print(\"ðŸ“ Sector:\", example['sector'])\n",
    "print(\"ðŸ‘¤ Author:\", example['author'])\n",
    "print(\"ðŸŽ­ Persona (auto-detected):\", example['persona'])\n",
    "print(\"ðŸ¤– Model:\", example['model_used'])\n",
    "print(\"ðŸ“‹ Prompt:\", example['prompt_type'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nðŸ“„ SOURCE (first 300 chars):\")\n",
    "print(example['document_content'][:300] + \"...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nðŸŽ¯ EXPECTED SUMMARY:\")\n",
    "print(example['expected_summary'][:500] + \"...\" if len(example['expected_summary']) > 500 else example['expected_summary'])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nðŸ¤– GENERATED SUMMARY:\")\n",
    "print(example['generated_summary'][:500] + \"...\" if len(example['generated_summary']) > 500 else example['generated_summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Metrics\n",
    "\n",
    "ROUGE measures n-gram overlap:\n",
    "- **ROUGE-1**: Word overlap\n",
    "- **ROUGE-2**: 2-word phrase overlap  \n",
    "- **ROUGE-L**: Longest common subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "\n",
    "reference = example['expected_summary']\n",
    "generated = example['generated_summary']\n",
    "\n",
    "scores = scorer.score(reference, generated)\n",
    "\n",
    "print(\"ðŸ“Š ROUGE Scores for Example 1:\\n\")\n",
    "for metric_name, metric_scores in scores.items():\n",
    "    print(f\"{metric_name.upper()}:\")\n",
    "    print(f\"  Precision: {metric_scores.precision:.4f}\")\n",
    "    print(f\"  Recall:    {metric_scores.recall:.4f}\")\n",
    "    print(f\"  F1:        {metric_scores.fmeasure:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTScore\n",
    "\n",
    "BERTScore uses neural embeddings to measure semantic similarity:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Local Model Setup\n\nCheck if RoBERTa-large model is available locally and configure environment:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\n\n# Check if local RoBERTa model exists\nlocal_model_path = Path(\"roberta-large\")\n\nif local_model_path.exists() and (local_model_path / \"config.json\").exists():\n    # Set HF_HOME to use local model files\n    os.environ['HF_HOME'] = str(local_model_path.parent.absolute())\n    print(f\"âœ“ Using local RoBERTa-large model from: {local_model_path.absolute()}\")\n    print(\"  This will speed up BERTScore evaluation and works offline.\")\nelse:\n    print(\"â„¹ Local RoBERTa model not found\")\n    print(\"  BERTScore will download from HuggingFace on first use.\")\n    print(\"  To use local model: run 'python setup_roberta.py'\")\nprint()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import bert_score\n",
    "    \n",
    "    print(\"Calculating BERTScore (may take a minute on first run)...\\n\")\n",
    "    \n",
    "    P, R, F1 = bert_score.score(\n",
    "        [example['generated_summary']], \n",
    "        [example['expected_summary']],\n",
    "        model_type='roberta-large',\n",
    "        lang='en',\n",
    "        rescale_with_baseline=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸ“Š BERTScore:\")\n",
    "    print(f\"  Precision: {P.item():.4f}\")\n",
    "    print(f\"  Recall:    {R.item():.4f}\")\n",
    "    print(f\"  F1:        {F1.item():.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ BERTScore F1 of {F1.item():.4f} indicates \", end=\"\")\n",
    "    if F1.item() > 0.9:\n",
    "        print(\"excellent semantic similarity\")\n",
    "    elif F1.item() > 0.85:\n",
    "        print(\"good semantic similarity\")\n",
    "    else:\n",
    "        print(\"fair semantic similarity\")\n",
    "    \n",
    "    bertscore_available = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš  BERTScore demo skipped: {e}\")\n",
    "    print(\"Install with: pip install bert-score\")\n",
    "    bertscore_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stylometric Analysis\n",
    "\n",
    "Extract writing style features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_config = {\n",
    "    'personas': {\n",
    "        'formal_analyst': 'data/personas/formal_analyst.txt',\n",
    "        'journalist': 'data/personas/journalist.txt',\n",
    "        'enthusiast': 'data/personas/enthusiast.txt'\n",
    "    }\n",
    "}\n",
    "style_analyzer = StyleAnalyzer(persona_config)\n",
    "\n",
    "# Extract features\n",
    "features = style_analyzer.extract_stylometric_features(example['generated_summary'])\n",
    "\n",
    "feature_names = [\n",
    "    'Function word rate',\n",
    "    'Avg sentence length (norm)',\n",
    "    'Type-token ratio',\n",
    "    'Comma rate',\n",
    "    'Period rate',\n",
    "    'Exclamation rate',\n",
    "    'Question rate',\n",
    "    'Pronoun rate',\n",
    "    'FK grade (norm)',\n",
    "    'Avg word length (norm)'\n",
    "]\n",
    "\n",
    "print(\"ðŸ“ Stylometric Features:\\n\")\n",
    "for name, value in zip(feature_names, features):\n",
    "    print(f\"{name:30s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persona Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build persona centroids\n",
    "print(\"Building persona centroids...\\n\")\n",
    "centroids = style_analyzer.build_persona_centroids(force_rebuild=True)\n",
    "print(f\"âœ“ Built centroids for {len(centroids)} personas\\n\")\n",
    "\n",
    "# Calculate similarity to each persona\n",
    "print(f\"Summary persona: {example['persona']}\")\n",
    "print(f\"\\nðŸ“Š Style Similarity to Each Persona:\\n\")\n",
    "\n",
    "similarities = {}\n",
    "for persona_id in centroids.keys():\n",
    "    similarity = style_analyzer.calculate_style_similarity(example['generated_summary'], persona_id)\n",
    "    similarities[persona_id] = similarity\n",
    "    marker = \"âœ“\" if persona_id == example['persona'] else \" \"\n",
    "    print(f\"  {marker} {persona_id:20s}: {similarity:.4f}\")\n",
    "\n",
    "best_match = max(similarities, key=similarities.get)\n",
    "print(f\"\\nðŸ† Best match: {best_match}\")\n",
    "if best_match == example['persona']:\n",
    "    print(\"âœ“ Correct!\")\n",
    "else:\n",
    "    print(f\"âœ— Mismatch (expected {example['persona']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Evaluation\n",
    "\n",
    "Run full evaluation on all records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "print(f\"Evaluating {len(records)} records...\\n\")\n",
    "\n",
    "content_config = {\n",
    "    'use_rouge': True,\n",
    "    'use_bertscore': bertscore_available,\n",
    "    'bertscore_model': 'roberta-large',\n",
    "    'use_bleurt': False\n",
    "}\n",
    "\n",
    "content_calc = ContentMetricsCalculator({'content': content_config})\n",
    "\n",
    "for i, rec in enumerate(records, 1):\n",
    "    print(f\"  [{i}/{len(records)}] {rec['document_title'][:50]}...\")\n",
    "    \n",
    "    # Calculate content metrics\n",
    "    content_metrics = content_calc.calculate_all_metrics(\n",
    "        source_text=rec['document_content'],\n",
    "        reference=rec['expected_summary'],\n",
    "        hypothesis=rec['generated_summary']\n",
    "    )\n",
    "    \n",
    "    # Calculate style similarity\n",
    "    style_sim = style_analyzer.calculate_style_similarity(\n",
    "        rec['generated_summary'], \n",
    "        rec['persona']\n",
    "    )\n",
    "    \n",
    "    # Calculate overall quality (70% content + 30% style)\n",
    "    content_quality = content_metrics.get('content_quality', 0.0)\n",
    "    style_score = style_sim if style_sim is not None else 0.0\n",
    "    overall_quality = (0.7 * content_quality) + (0.3 * style_score)\n",
    "    \n",
    "    results.append({\n",
    "        'file': rec['_source_file'],\n",
    "        'title': rec['document_title'][:40],\n",
    "        'sector': rec['sector'],\n",
    "        'persona': rec['persona'],\n",
    "        'model': rec['model_used'],\n",
    "        **content_metrics,\n",
    "        'style_similarity': style_sim,\n",
    "        'overall_quality': overall_quality\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nâœ“ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“Š EVALUATION RESULTS\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display key columns\n",
    "display_cols = ['title', 'persona', 'overall_quality', 'rouge1_f', 'rougeLsum_f']\n",
    "if bertscore_available and 'bertscore_f1' in df.columns:\n",
    "    display_cols.append('bertscore_f1')\n",
    "display_cols.append('style_similarity')\n",
    "\n",
    "print(df[display_cols].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nðŸ“ˆ SUMMARY STATISTICS\\n\")\n",
    "\n",
    "print(\"Overall Quality (70% content + 30% style):\")\n",
    "print(f\"  Overall Quality:  {df['overall_quality'].mean():.4f} Â± {df['overall_quality'].std():.4f}\")\n",
    "print(f\"  Min: {df['overall_quality'].min():.4f}, Max: {df['overall_quality'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nContent Quality:\")\n",
    "print(f\"  Content Quality:  {df['content_quality'].mean():.4f} Â± {df['content_quality'].std():.4f}\")\n",
    "print(f\"  ROUGE-1 F1:       {df['rouge1_f'].mean():.4f} Â± {df['rouge1_f'].std():.4f}\")\n",
    "print(f\"  ROUGE-Lsum F1:    {df['rougeLsum_f'].mean():.4f} Â± {df['rougeLsum_f'].std():.4f}\")\n",
    "\n",
    "if bertscore_available and 'bertscore_f1' in df.columns:\n",
    "    print(f\"  BERTScore F1:     {df['bertscore_f1'].mean():.4f} Â± {df['bertscore_f1'].std():.4f}\")\n",
    "\n",
    "print(f\"\\nStyle Fidelity:\")\n",
    "print(f\"  Style Similarity: {df['style_similarity'].mean():.4f} Â± {df['style_similarity'].std():.4f}\")\n",
    "\n",
    "print(f\"\\nBy Persona:\")\n",
    "for persona in sorted(df['persona'].unique()):\n",
    "    persona_df = df[df['persona'] == persona]\n",
    "    print(f\"  {persona:20s}: {len(persona_df)} items\")\n",
    "    print(f\"    Overall:  {persona_df['overall_quality'].mean():.3f}\")\n",
    "    print(f\"    Content:  {persona_df['content_quality'].mean():.3f}\")\n",
    "    print(f\"    ROUGE-L:  {persona_df['rougeLsum_f'].mean():.3f}\")\n",
    "    print(f\"    Style:    {persona_df['style_similarity'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\nBy Sector:\")\n",
    "for sector in sorted(df['sector'].unique()):\n",
    "    sector_df = df[df['sector'] == sector]\n",
    "    print(f\"  {sector:20s}: {len(sector_df)} items, overall={sector_df['overall_quality'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    sns.set_style('whitegrid')\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Overall Quality vs ROUGE\n",
    "    for persona in df['persona'].unique():\n",
    "        persona_df = df[df['persona'] == persona]\n",
    "        axes[0, 0].scatter(persona_df['rougeLsum_f'], persona_df['overall_quality'], \n",
    "                       label=persona, alpha=0.7, s=150)\n",
    "    axes[0, 0].set_xlabel('ROUGE-Lsum F1')\n",
    "    axes[0, 0].set_ylabel('Overall Quality')\n",
    "    axes[0, 0].set_title('ROUGE vs Overall Quality')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Content vs Style\n",
    "    for persona in df['persona'].unique():\n",
    "        persona_df = df[df['persona'] == persona]\n",
    "        axes[0, 1].scatter(persona_df['content_quality'], persona_df['style_similarity'], \n",
    "                       label=persona, alpha=0.7, s=150)\n",
    "    axes[0, 1].set_xlabel('Content Quality')\n",
    "    axes[0, 1].set_ylabel('Style Similarity')\n",
    "    axes[0, 1].set_title('Content Quality vs Style Fidelity')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overall quality by persona\n",
    "    df.boxplot(column='overall_quality', by='persona', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Overall Quality by Persona')\n",
    "    axes[1, 0].set_ylabel('Overall Quality')\n",
    "    axes[1, 0].set_xlabel('Persona')\n",
    "    \n",
    "    # Overall quality by sector\n",
    "    df.boxplot(column='overall_quality', by='sector', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Overall Quality by Sector')\n",
    "    axes[1, 1].set_ylabel('Overall Quality')\n",
    "    axes[1, 1].set_xlabel('Sector')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/evaluation_viz.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\nâœ“ Saved visualization to outputs/evaluation_viz.png\")\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš  Matplotlib not available\")\n",
    "    print(\"Install with: pip install matplotlib seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Add more JSON files** to `data/` directory\n",
    "2. **Run full evaluation**: `python -m src.eval_runner`\n",
    "3. **Check results**: `outputs/per_item_metrics.csv`\n",
    "4. **Customize personas**: Edit `data/personas/*.txt`\n",
    "\n",
    "### Interpreting Scores\n",
    "\n",
    "**Overall Quality (Combined Metric):**\n",
    "- Overall Quality > 0.6: Good summary (70% content + 30% style)\n",
    "- Overall Quality > 0.7: Excellent summary\n",
    "\n",
    "**Content Quality:**\n",
    "- Content Quality > 0.6: Good content coverage (weighted ROUGE + BERTScore)\n",
    "- ROUGE-Lsum F1 > 0.6: Good overlap with reference\n",
    "- BERTScore F1 > 0.85: Strong semantic match\n",
    "\n",
    "**Style Fidelity:**\n",
    "- Style similarity > 0.7: Good persona match\n",
    "- Style similarity > 0.8: Strong persona match\n",
    "\n",
    "### Metric Weighting\n",
    "\n",
    "The overall quality metric combines:\n",
    "- **70% Content Quality**: Weighted average of ROUGE (40%), BERTScore (30%), BLEURT (30%)\n",
    "- **30% Style Similarity**: Jensen-Shannon divergence of stylometric features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}