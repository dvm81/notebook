{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Evaluation Demo - Complete Metrics Including BLEURT\n",
    "\n",
    "This notebook demonstrates **all evaluation metrics** including BLEURT.\n",
    "\n",
    "## Metrics Covered:\n",
    "1. ‚úÖ **ROUGE** (n-gram overlap)\n",
    "2. ‚úÖ **BERTScore** (semantic similarity)\n",
    "3. ‚úÖ **BLEURT** (learned human-correlated metric) ‚≠ê\n",
    "4. ‚úÖ **Style Similarity** (persona matching)\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#setup)\n",
    "2. [Single Example with All Metrics](#single)\n",
    "3. [Batch Evaluation](#batch)\n",
    "4. [Results & Analysis](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 1. Setup\n",
    "\n",
    "Install required packages if not already installed:\n",
    "```bash\n",
    "pip install rouge-score bert-score tensorflow\n",
    "pip install git+https://github.com/google-research/bleurt.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from src.io_utils import load_jsonl, load_persona_assignments\n",
    "from src.style_features import StyleAnalyzer\n",
    "\n",
    "print(\"‚úì Basic imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and data\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "records = list(load_jsonl('data/input.jsonl'))\n",
    "persona_map = load_persona_assignments('data/persona_assignments.csv')\n",
    "\n",
    "print(f\"‚úì Loaded {len(records)} articles\")\n",
    "print(f\"‚úì Loaded {len(persona_map)} persona assignments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ROUGE\n",
    "from rouge_score import rouge_scorer\n",
    "rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "print(\"‚úì ROUGE initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERTScore\n",
    "try:\n",
    "    import bert_score\n",
    "    print(\"‚úì BERTScore available\")\n",
    "    bertscore_available = True\n",
    "except:\n",
    "    print(\"‚úó BERTScore not available (pip install bert-score)\")\n",
    "    bertscore_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BLEURT - THIS IS THE IMPORTANT PART!\n",
    "print(\"=\"*80)\n",
    "print(\"INITIALIZING BLEURT\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNote: First run will download models (~1GB) and may take 2-5 minutes.\\n\")\n",
    "\n",
    "try:\n",
    "    from bleurt import score as bleurt_score_module\n",
    "    \n",
    "    # Get checkpoint from config\n",
    "    bleurt_checkpoint = config['content'].get('bleurt_checkpoint', 'BLEURT-20-D12')\n",
    "    print(f\"Loading BLEURT checkpoint: {bleurt_checkpoint}\")\n",
    "    print(\"Please wait...\\n\")\n",
    "    \n",
    "    # Initialize scorer\n",
    "    bleurt_scorer = bleurt_score_module.BleurtScorer(bleurt_checkpoint)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"‚úì ‚úì ‚úì  BLEURT SUCCESSFULLY INITIALIZED  ‚úì ‚úì ‚úì\")\n",
    "    print(\"=\"*80)\n",
    "    bleurt_available = True\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(\"=\"*80)\n",
    "    print(\"‚úó BLEURT NOT AVAILABLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"\\nTo install BLEURT:\")\n",
    "    print(\"  1. pip install tensorflow>=2.0.0\")\n",
    "    print(\"  2. pip install git+https://github.com/google-research/bleurt.git\")\n",
    "    print(\"\\nNote: BLEURT requires TensorFlow which can be large.\")\n",
    "    bleurt_available = False\n",
    "    bleurt_scorer = None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"=\"*80)\n",
    "    print(\"‚úó BLEURT INITIALIZATION FAILED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"\\nCommon issues:\")\n",
    "    print(\"  - TensorFlow version incompatibility\")\n",
    "    print(\"  - Network issues downloading models\")\n",
    "    print(\"  - Insufficient disk space (~1GB needed)\")\n",
    "    bleurt_available = False\n",
    "    bleurt_scorer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Style Analyzer\n",
    "style_analyzer = StyleAnalyzer(config)\n",
    "style_analyzer.build_persona_centroids()\n",
    "print(\"‚úì Style analyzer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of available metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"METRICS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  {'‚úì' if True else '‚úó'} ROUGE\")\n",
    "print(f\"  {'‚úì' if bertscore_available else '‚úó'} BERTScore\")\n",
    "print(f\"  {'‚úì' if bleurt_available else '‚úó'} BLEURT  {'<-- PRIMARY METRIC' if bleurt_available else '<-- UNAVAILABLE'}\")\n",
    "print(f\"  {'‚úì' if True else '‚úó'} Style Similarity\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='single'></a>\n",
    "## 2. Single Example with All Metrics\n",
    "\n",
    "Let's evaluate one article with all metrics to see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first example\n",
    "example = records[0]\n",
    "reference = example['expected_summary']\n",
    "generated = example['agented_summary']\n",
    "persona = persona_map.get(example['write_id'])\n",
    "\n",
    "print(\"üì∞ Article:\", example['document_title'])\n",
    "print(\"üé≠ Persona:\", persona)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REFERENCE SUMMARY:\")\n",
    "print(reference)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATED SUMMARY:\")\n",
    "print(generated)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROUGE\n",
    "rouge_scores = rouge_scorer_obj.score(reference, generated)\n",
    "\n",
    "print(\"\\nüìä ROUGE SCORES:\")\n",
    "print(f\"  ROUGE-1 F1:    {rouge_scores['rouge1'].fmeasure:.4f}\")\n",
    "print(f\"  ROUGE-2 F1:    {rouge_scores['rouge2'].fmeasure:.4f}\")\n",
    "print(f\"  ROUGE-Lsum F1: {rouge_scores['rougeLsum'].fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BERTScore\n",
    "if bertscore_available:\n",
    "    P, R, F1 = bert_score.score([generated], [reference], \n",
    "                                 model_type='distilbert-base-uncased', verbose=False)\n",
    "    bert_f1 = F1.item()\n",
    "    print(f\"\\nüìä BERTSCORE:\")\n",
    "    print(f\"  F1: {bert_f1:.4f}\")\n",
    "else:\n",
    "    bert_f1 = None\n",
    "    print(\"\\n‚ö† BERTScore not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEURT - THE KEY METRIC!\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üåü BLEURT SCORE (HUMAN-CORRELATED METRIC) üåü\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if bleurt_available and bleurt_scorer is not None:\n",
    "    print(\"\\nCalculating BLEURT score...\")\n",
    "    \n",
    "    bleurt_scores = bleurt_scorer.score(\n",
    "        references=[reference],\n",
    "        candidates=[generated]\n",
    "    )\n",
    "    \n",
    "    bleurt_val = bleurt_scores[0]\n",
    "    \n",
    "    print(\"\\n‚úì BLEURT calculation complete!\\n\")\n",
    "    print(f\"üìä BLEURT Score: {bleurt_val:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if bleurt_val > 0.6:\n",
    "        quality = \"EXCELLENT\"\n",
    "        color = \"üü¢\"\n",
    "    elif bleurt_val > 0.4:\n",
    "        quality = \"GOOD\"\n",
    "        color = \"üü°\"\n",
    "    elif bleurt_val > 0.2:\n",
    "        quality = \"FAIR\"\n",
    "        color = \"üü†\"\n",
    "    else:\n",
    "        quality = \"NEEDS IMPROVEMENT\"\n",
    "        color = \"üî¥\"\n",
    "    \n",
    "    print(f\"\\n{color} Quality Assessment: {quality}\")\n",
    "    print(\"\\nüí° BLEURT Interpretation:\")\n",
    "    print(\"   BLEURT is trained on human judgments of summary quality.\")\n",
    "    print(\"   Scores range from -1 to +1, with higher being better.\")\n",
    "    print(\"   Typical good summaries score 0.4-0.7.\")\n",
    "    \n",
    "else:\n",
    "    bleurt_val = None\n",
    "    print(\"\\n‚úó BLEURT not available\")\n",
    "    print(\"   Please install: pip install tensorflow && pip install git+https://github.com/google-research/bleurt.git\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Style Similarity\n",
    "style_sim = style_analyzer.calculate_style_similarity(generated, persona)\n",
    "\n",
    "print(f\"\\nüìä STYLE SIMILARITY:\")\n",
    "print(f\"  Score: {style_sim:.4f}\")\n",
    "print(f\"  Target persona: {persona}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON TABLE\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL METRICS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Metric':<20} {'Score':<10} {'Interpretation'}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'ROUGE-1 F1':<20} {rouge_scores['rouge1'].fmeasure:<10.4f} {'Word overlap'}\")\n",
    "print(f\"{'ROUGE-2 F1':<20} {rouge_scores['rouge2'].fmeasure:<10.4f} {'Phrase overlap'}\")\n",
    "print(f\"{'ROUGE-Lsum F1':<20} {rouge_scores['rougeLsum'].fmeasure:<10.4f} {'Sentence structure'}\")\n",
    "\n",
    "if bert_f1 is not None:\n",
    "    print(f\"{'BERTScore F1':<20} {bert_f1:<10.4f} {'Semantic similarity'}\")\n",
    "\n",
    "if bleurt_val is not None:\n",
    "    print(f\"{'BLEURT':<20} {bleurt_val:<10.4f} {'Human-like quality ‚≠ê'}\")\n",
    "\n",
    "print(f\"{'Style Similarity':<20} {style_sim:<10.4f} {'Persona matching'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='batch'></a>\n",
    "## 3. Batch Evaluation\n",
    "\n",
    "Now let's evaluate all articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all articles\n",
    "results = []\n",
    "\n",
    "print(f\"\\nEvaluating {len(records)} articles...\")\n",
    "print(\"\\nMetrics being calculated:\")\n",
    "print(f\"  ‚úì ROUGE (fast)\")\n",
    "if bertscore_available:\n",
    "    print(f\"  ‚úì BERTScore (moderate)\")\n",
    "if bleurt_available:\n",
    "    print(f\"  ‚úì BLEURT (slower but most accurate) ‚≠ê\")\n",
    "print(f\"  ‚úì Style Similarity (fast)\")\n",
    "print()\n",
    "\n",
    "for i, rec in enumerate(records, 1):\n",
    "    print(f\"Processing {i}/{len(records)}: {rec['document_title'][:50]}...\")\n",
    "    \n",
    "    ref = rec['expected_summary']\n",
    "    gen = rec['agented_summary']\n",
    "    pers = persona_map.get(rec['write_id'])\n",
    "    \n",
    "    # ROUGE\n",
    "    rouge = rouge_scorer_obj.score(ref, gen)\n",
    "    \n",
    "    # BERTScore\n",
    "    bert = None\n",
    "    if bertscore_available:\n",
    "        try:\n",
    "            _, _, F = bert_score.score([gen], [ref], model_type='distilbert-base-uncased', verbose=False)\n",
    "            bert = F.item()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # BLEURT\n",
    "    bleurt = None\n",
    "    if bleurt_available and bleurt_scorer is not None:\n",
    "        try:\n",
    "            scores = bleurt_scorer.score(references=[ref], candidates=[gen])\n",
    "            bleurt = scores[0]\n",
    "            print(f\"  ‚Üí BLEURT: {bleurt:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚Üí BLEURT failed: {e}\")\n",
    "    \n",
    "    # Style\n",
    "    style = style_analyzer.calculate_style_similarity(gen, pers)\n",
    "    \n",
    "    results.append({\n",
    "        'article': rec['document_title'][:50],\n",
    "        'persona': pers,\n",
    "        'rouge1_f1': rouge['rouge1'].fmeasure,\n",
    "        'rouge2_f1': rouge['rouge2'].fmeasure,\n",
    "        'rougeLsum_f1': rouge['rougeLsum'].fmeasure,\n",
    "        'bertscore_f1': bert,\n",
    "        'bleurt': bleurt,\n",
    "        'style_sim': style\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n‚úì Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='results'></a>\n",
    "## 4. Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"COMPLETE RESULTS TABLE\")\n",
    "print(\"=\"*120 + \"\\n\")\n",
    "\n",
    "# Select columns to display\n",
    "display_cols = ['article', 'persona', 'rouge1_f1', 'rouge2_f1', 'rougeLsum_f1']\n",
    "if bertscore_available and df['bertscore_f1'].notna().any():\n",
    "    display_cols.append('bertscore_f1')\n",
    "if bleurt_available and df['bleurt'].notna().any():\n",
    "    display_cols.append('bleurt')\n",
    "display_cols.append('style_sim')\n",
    "\n",
    "print(df[display_cols].to_string(index=False, float_format=lambda x: f'{x:.4f}' if pd.notna(x) else 'N/A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Mean Scores:\")\n",
    "print(f\"  ROUGE-1 F1:       {df['rouge1_f1'].mean():.4f}\")\n",
    "print(f\"  ROUGE-2 F1:       {df['rouge2_f1'].mean():.4f}\")\n",
    "print(f\"  ROUGE-Lsum F1:    {df['rougeLsum_f1'].mean():.4f}\")\n",
    "\n",
    "if bertscore_available and df['bertscore_f1'].notna().any():\n",
    "    print(f\"  BERTScore F1:     {df['bertscore_f1'].mean():.4f}\")\n",
    "\n",
    "if bleurt_available and df['bleurt'].notna().any():\n",
    "    print(f\"  BLEURT:           {df['bleurt'].mean():.4f} ‚≠ê (Most human-like)\")\n",
    "\n",
    "print(f\"  Style Similarity: {df['style_sim'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nStandard Deviations:\")\n",
    "print(f\"  ROUGE-Lsum F1:    {df['rougeLsum_f1'].std():.4f}\")\n",
    "if bleurt_available and df['bleurt'].notna().any():\n",
    "    print(f\"  BLEURT:           {df['bleurt'].std():.4f}\")\n",
    "print(f\"  Style Similarity: {df['style_sim'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEURT-specific analysis\n",
    "if bleurt_available and df['bleurt'].notna().any():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üåü BLEURT ANALYSIS üåü\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    print(\"BLEURT Score Distribution:\")\n",
    "    print(f\"  Min:    {df['bleurt'].min():.4f}\")\n",
    "    print(f\"  Max:    {df['bleurt'].max():.4f}\")\n",
    "    print(f\"  Mean:   {df['bleurt'].mean():.4f}\")\n",
    "    print(f\"  Median: {df['bleurt'].median():.4f}\")\n",
    "    \n",
    "    # Quality breakdown\n",
    "    excellent = (df['bleurt'] > 0.6).sum()\n",
    "    good = ((df['bleurt'] > 0.4) & (df['bleurt'] <= 0.6)).sum()\n",
    "    fair = ((df['bleurt'] > 0.2) & (df['bleurt'] <= 0.4)).sum()\n",
    "    poor = (df['bleurt'] <= 0.2).sum()\n",
    "    \n",
    "    print(\"\\nQuality Breakdown:\")\n",
    "    print(f\"  üü¢ Excellent (>0.6):  {excellent} articles\")\n",
    "    print(f\"  üü° Good (0.4-0.6):    {good} articles\")\n",
    "    print(f\"  üü† Fair (0.2-0.4):    {fair} articles\")\n",
    "    print(f\"  üî¥ Poor (<0.2):       {poor} articles\")\n",
    "    \n",
    "    # Best and worst\n",
    "    best_idx = df['bleurt'].idxmax()\n",
    "    worst_idx = df['bleurt'].idxmin()\n",
    "    \n",
    "    print(\"\\nBest BLEURT Score:\")\n",
    "    print(f\"  Article: {df.loc[best_idx, 'article']}\")\n",
    "    print(f\"  BLEURT:  {df.loc[best_idx, 'bleurt']:.4f}\")\n",
    "    \n",
    "    print(\"\\nWorst BLEURT Score:\")\n",
    "    print(f\"  Article: {df.loc[worst_idx, 'article']}\")\n",
    "    print(f\"  BLEURT:  {df.loc[worst_idx, 'bleurt']:.4f}\")\n",
    "    \n",
    "    # Correlation with other metrics\n",
    "    print(\"\\nCorrelation with other metrics:\")\n",
    "    print(f\"  BLEURT vs ROUGE-Lsum:  {df['bleurt'].corr(df['rougeLsum_f1']):.4f}\")\n",
    "    if bertscore_available and df['bertscore_f1'].notna().any():\n",
    "        print(f\"  BLEURT vs BERTScore:   {df['bleurt'].corr(df['bertscore_f1']):.4f}\")\n",
    "    print(f\"  BLEURT vs Style Sim:   {df['bleurt'].corr(df['style_sim']):.4f}\")\n",
    "    \n",
    "    print(\"\\nüí° Interpretation:\")\n",
    "    print(\"   High BLEURT scores indicate summaries that humans would rate as high quality.\")\n",
    "    print(\"   BLEURT considers fluency, coherence, and meaning preservation.\")\n",
    "    print(\"   Unlike ROUGE, BLEURT can recognize good paraphrases and penalize awkward text.\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö† BLEURT analysis not available\")\n",
    "    print(\"   Install BLEURT to see human-correlated quality scores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_file = 'outputs/notebook_evaluation_results.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úì Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated evaluation with **all metrics including BLEURT**.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **ROUGE** measures word/phrase overlap - good baseline\n",
    "2. **BERTScore** captures semantic similarity - handles paraphrasing\n",
    "3. **BLEURT** ‚≠ê - trained on human judgments, most reliable indicator of quality\n",
    "4. **Style Similarity** - measures persona matching\n",
    "\n",
    "### Why BLEURT Matters:\n",
    "\n",
    "- **Correlates with human judgments** better than ROUGE or BERTScore\n",
    "- **Considers fluency and coherence**, not just word overlap\n",
    "- **Recognizes quality** that humans would appreciate\n",
    "- **Range**: -1 (poor) to +1 (excellent), typically 0.3-0.7 for good summaries\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Analyze low-BLEURT articles to understand failures\n",
    "2. Use BLEURT to compare different summarization approaches\n",
    "3. Optimize for BLEURT scores to improve human-perceived quality\n",
    "\n",
    "---\n",
    "\n",
    "**For production evaluation**, run:\n",
    "```bash\n",
    "python -m src.eval_runner  # Uses all metrics including BLEURT\n",
    "python -m src.report       # Generates comprehensive report\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
